# Mysl stroje: LLM ğŸ¤–

<Callout type="info">
**CÃ­l mise:** OtevÅ™Ã­t mozek ChatGPT a podÃ­vat se, jak myslÃ­ (v ÄÃ­slech).
â³ **ÄŒas ÄtenÃ­:** 20 min | ğŸ§ª **[3] Laby souÄÃ¡stÃ­**
</Callout>

<VideoSwitcher alternatives={[{"id":"LPZh9BOjkQs","title":"LLMs StruÄnÄ› (3B1B)"},{"id":"eMlx5fFnoYc","title":"Attention Mechanismus (3B1B)"},{"id":"zjkBMFhNj_g","title":"Ãšvod do LLM (Andrej Karpathy)"},{"id":"RKw_x8dYq-8","title":"Co je LLM? (CZ 2025)"}]} />

## âš¡ NepÅ™Ã­jemnÃ¡ pravda

**LLM nemajÃ­ lidskÃ© porozumÄ›nÃ­.**

PÅ™eÄti si to znovu. Stroj, kterÃ½ pÃ­Å¡e poezii, ladÃ­ kÃ³d a sloÅ¾il advokÃ¡tnÃ­ zkouÅ¡ky, â€nerozumÃ­" slovÅ¯m tak jako ty. Generuje text pÅ™edpovÃ­dÃ¡nÃ­m tokenÅ¯ ze vzorÅ¯ v datechâ€”nejsofistikovanÄ›jÅ¡Ã­ hra na *automatickÃ© doplÅˆovÃ¡nÃ­*, jakÃ¡ kdy vznikla.

KaÅ¾dÃ© slovo, kterÃ© ChatGPT vygeneruje, je statistickÃ¡ sÃ¡zka. Predikce. Hazard na to, kterÃ½ token by mÄ›l pÅ™ijÃ­t dÃ¡l, zaloÅ¾enÃ½ na miliardÃ¡ch nauÄenÃ½ch parametrÅ¯ (pÅ™esnÃ© poÄty obvykle nejsou veÅ™ejnÃ©).

**A pÅ™esto... to funguje.**

Video vÃ½Å¡e od 3Blue1Brown ti ukÃ¡Å¾e pÅ™esnÄ› jak. PodÃ­vej se na nÄ›jâ€”uÅ¾ nikdy nebudeÅ¡ vidÄ›t ChatGPT stejnÄ›.

---

## ğŸ”® 1. Hra na pÅ™edpovÃ­dÃ¡nÃ­

PÅ™edstav si, Å¾e C-3PO pÅ™eklÃ¡dÃ¡ prastarÃ½ sithskÃ½ dialekt. NecÃ­tÃ­ v tÄ›ch slovech hnÄ›v. VypoÄÃ­tÃ¡vÃ¡ **pravdÄ›podobnost**, kterÃ© slovo obvykle nÃ¡sleduje po tom pÅ™edchozÃ­m.

LLM je pÅ™esnÄ› tohle. Je to stroj vycviÄenÃ½ na masivnÃ­ch textovÃ½ch korpusechâ€”webovÃ½ch strÃ¡nkÃ¡ch, knihÃ¡ch, kÃ³du a dalÅ¡Ã­ch zdrojÃ­châ€”aby hrÃ¡l jednu jednoduchou hru: **â€UhÃ¡dni dalÅ¡Ã­ token."**

<Diagram type="llm-next-token" />

<ConceptCard title="Predikce dalÅ¡Ã­ho tokenu" icon="ğŸ”®" jediQuote="SoustÅ™eÄ se na okamÅ¾ik. Co pÅ™ijde dÃ¡l?">
*   **VstupnÃ­ kontext:** "Obloha je..."
*   **VÃ½poÄet:** ModrÃ¡ (90 %), Å edÃ¡ (5 %), ZelenÃ¡ (1 %).
*   **Akce:** AI hodÃ­ kostkou na zÃ¡kladÄ› tÄ›chto pravdÄ›podobnostÃ­ a vybere "ModrÃ¡".
</ConceptCard>

---

## ğŸ—ï¸ 2. Motor: Pozornost je vÅ¡e, co potÅ™ebujeÅ¡

PÅ™ed rokem 2017 mÄ›la AI problÃ©my s dlouhÃ½mi vÄ›tami. NeÅ¾ se dostala na konec, zapomnÄ›la zaÄÃ¡tek.
Pak vÃ½zkumnÃ­ci pÅ™edstavili architekturu **Transformer** (Vaswani et al., 2017, Google).

Tou tajnou pÅ™Ã­sadou je **"Sebe-pozornost" (Self-Attention)**.
PÅ™edstav si, Å¾e ÄteÅ¡ vÄ›tu: *"ZvÃ­Å™e nepÅ™eÅ¡lo ulici, protoÅ¾e bylo pÅ™Ã­liÅ¡ unavenÃ©."*

*   **StarÃ¡ AI:** ÄŒte zleva doprava. Zmate ji "bylo".
*   **Transformer:** PodÃ­vÃ¡ se na "bylo" a spojÃ­ si ho se "zvÃ­Å™e" (ne "ulicÃ­") dÃ­ky kontextu "unavenÃ©".

<Diagram type="transformer-architecture-simplified" />

VÄ›nuje â€pozornost" vÅ¡em slovÅ¯m najednou a vÃ¡Å¾Ã­ jejich dÅ¯leÅ¾itost. Proto tak efektivnÄ› pracuje s kontextem.

---

## ğŸ”¢ 3. NeÄte slova. ÄŒte ÄÃ­sla.

Tady je tajemstvÃ­: PoÄÃ­taÄe neumÃ­ ÄÃ­st. UmÃ­ jen poÄÃ­tat.
NeÅ¾ LLM uvidÃ­ tvÅ¯j prompt, rozsekÃ¡ ho na malÃ© kousky zvanÃ© **Tokeny**.

PÅ™edstav si tokeny jako **KostiÄky Lega**.
*   ğŸ **Jablko** = 1 KostiÄka
*   ğŸ§± **Ing** (pÅ™Ã­pona) = 1 KostiÄka
*   â¬œ **Mezera** = 1 KostiÄka

Zhruba platÃ­: **1 000 tokenÅ¯ â‰ˆ 750 slov**.
AI pÅ™evede tyto tokeny na ÄÃ­sla (napÅ™. `18342`), provede na nich matematickÃ© operace a vÃ½sledek pÅ™evede zpÄ›t na slova.

<Diagram type="tokenization-viz" />

---

## ğŸ”¬ Lab 1: Pohled TokenizÃ©ru

Je Äas spatÅ™it Matrix. Ale tady je tajemstvÃ­: AI ve skuteÄnosti *nevidÃ­* svou vlastnÃ­ tokenizaciâ€”mÅ¯Å¾e ji pouze odhadovat. PouÅ¾ijme radÄ›ji skuteÄnÃ½ nÃ¡stroj.

**CÃ­l:** Vizualizovat skuteÄnou tokenizaci a pochopit, proÄ na tom zÃ¡leÅ¾Ã­.

**ÄŒÃ¡st A: PouÅ¾ij OficiÃ¡lnÃ­ NÃ¡stroj**

1. OtevÅ™i OpenAI Tokenizer: [platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)
2. VloÅ¾ toto slovo: `NejneobhospodaÅ™ovÃ¡vatelnÄ›jÅ¡Ã­mi`
3. SpoÄÃ­tej tokeny (mÄ›lo by bÃ½t ~8-12 podle modelu)

**ÄŒÃ¡st B: PodÃ­vej se, proÄ na tom zÃ¡leÅ¾Ã­**

TeÄ otestuj tyto vstupy a sleduj poÄet tokenÅ¯:

| Vstup | Co sledovat |
|-------|------------------|
| `ahoj` | ÄŒasto 1 token |
| `Ahoj` | ÄŒasto 1 token (mÅ¯Å¾e se liÅ¡it od malÃ©ho) |
| ` ahoj` | MÅ¯Å¾e bÃ½t 1-2 tokeny (zpracovÃ¡nÃ­ mezer se liÅ¡Ã­) |
| `ahoj!` | ÄŒasto 2 tokeny |
| `ğŸ‰` | 1-3 tokeny (tokenizace emoji se liÅ¡Ã­) |

**ÄŒÃ¡st C: PorovnÃ¡nÃ­ s AI**

Zeptej se AI:
```text
Kolik tokenÅ¯ mÃ¡ pÅ™ibliÅ¾nÄ› slovo "NejneobhospodaÅ™ovÃ¡vatelnÄ›jÅ¡Ã­mi"?
```

Porovnej jejÃ­ odhad se skuteÄnÃ½m tokenizÃ©rem. Jak pÅ™esnÃ¡ byla?

**ğŸ’¡ Aha Moment:** â€AI nevidÃ­ pÃ­smenaâ€”vidÃ­ shluky (tokeny). Mezera na zaÄÃ¡tku Äasto mÄ›nÃ­ tokenizaci; emoji a interpunkce se mohou mapovat na vÃ­ce tokenÅ¯. Proto jsou Ãºlohy na Ãºrovni znakÅ¯ (jako pravopis nebo poÄÃ­tÃ¡nÃ­ pÃ­smen) tÄ›Å¾Å¡Ã­â€”model pracuje s tokeny, ne s jednotlivÃ½mi znaky."

<LabComplete labId="lab-tokenizer-view" />

---

## ğŸ§  4. KontextovÃ© okno (KrÃ¡tkodobÃ¡ pamÄ›Å¥)

UÅ¾ jsi nÄ›kdy mluvil s ChatGPT tak dlouho, Å¾e zapomnÄ›la, co jsi Å™Ã­kal na zaÄÃ¡tku?
To je kvÅ¯li **KontextovÃ©mu oknu**.

PÅ™edstav si Jediho odrÃ¡Å¾ejÃ­cÃ­ho stÅ™ely z blasterÅ¯. MÅ¯Å¾e se soustÅ™edit jen na urÄitÃ½ poÄet najednou. Pokud jich pÅ™ijde pÅ™Ã­liÅ¡ mnoho, pustÃ­ ty dÅ™Ã­vÄ›jÅ¡Ã­, aby se soustÅ™edil na novÃ©.

<Diagram type="context-window" />

*   **Vstup:** VÅ¡e, co napÃ­Å¡eÅ¡ + vÅ¡e, co AI zatÃ­m odpovÄ›dÄ›la.
*   **Limit:** KaÅ¾dÃ½ model mÃ¡ maximÃ¡lnÃ­ limit (napÅ™. 128k tokenÅ¯).
*   **PÅ™eteÄenÃ­:** KdyÅ¾ pÅ™ekroÄÃ­Å¡ limit, systÃ©m musÃ­ zredukovat kontextâ€”Äasto vypuÅ¡tÄ›nÃ­m starÅ¡Ã­ch tokenÅ¯ nebo sumarizacÃ­â€”takÅ¾e dÅ™Ã­vÄ›jÅ¡Ã­ detaily mohou bÃ½t ztraceny.

---

## ğŸŒ¡ï¸ 5. Teplota: Od Droida k BÃ¡snÃ­kovi

Jak udÄ›lÃ¡me z matematickÃ©ho stroje kreativce? UpravÃ­me **Teplotu**.
Tento parametr Å™Ã­dÃ­, jak "riskantnÃ­" je AI pÅ™i vÃ½bÄ›ru dalÅ¡Ã­ho tokenu.

<Diagram type="temperature-scale" />

*   â„ï¸ **Teplota 0.0 (SithskÃ¡ logika):** VÅ¾dy vybere nejpravdÄ›podobnÄ›jÅ¡Ã­ slovo. PÅ™esnÃ©, chladnÃ©, robotickÃ©. (KÃ³dovÃ¡nÃ­, Matematika).
*   ğŸ”¥ **Teplota 1.0 (Intuice JediÅ¯):** Riskuje. VybÃ­rÃ¡ mÃ©nÄ› pravdÄ›podobnÃ¡ slova. KreativnÃ­, chaotickÃ©. (Poezie, NÃ¡pady).

---

## ğŸ”¬ Lab 2: DJ Teploty

VÄ›tÅ¡ina chatovacÃ­ch aplikacÃ­ neukazuje posuvnÃ­k teplotyâ€”to je nastavenÃ­ API/modelu. MÅ¯Å¾eme vÅ¡ak aproximovat â€deterministickÃ© vs kreativnÃ­" chovÃ¡nÃ­ pomocÃ­ instrukcÃ­. Sleduj, jak se stejnÃ½ obsah transformuje podle stylu.

<Callout type="info">
**PoznÃ¡mka:** Tento lab pouÅ¾Ã­vÃ¡ prompting k *simulaci* efektÅ¯ teploty. SkuteÄnÃ¡ teplota je sampling parametr nastavenÃ½ na Ãºrovni APIâ€”tvÃ© chat instrukce mohou napodobit efekt, ale ve skuteÄnosti nemÄ›nÃ­ nastavenÃ­ nÃ¡hodnosti modelu.
</Callout>

**CÃ­l:** VidÄ›t, jak teplota ovlivÅˆuje tÃ³n, volbu slov a strukturu.

**Prompt:**
```text
PopiÅ¡, jak znÃ­ dÃ©Å¡Å¥. NapiÅ¡ 3 verze:

ğŸ¤– VERZE A (ROBOT - Teplota 0.0):
Jsi autor technickÃ© dokumentace. BuÄ extrÃ©mnÄ› pÅ™esnÃ½, faktickÃ½, klinickÃ½.
Å½Ã¡dnÃ© metafory. Å½Ã¡dnÃ© emoce. ÄŒistÃ¡ informace.

ğŸ‘¤ VERZE B (ÄŒLOVÄšK - Teplota 0.5):
Jsi pÅ™Ã¡telskÃ½ novinÃ¡Å™. BuÄ jasnÃ½ a poutavÃ½. PouÅ¾ij jedno nebo dvÄ› jednoduchÃ¡ pÅ™irovnÃ¡nÃ­.
VyvÃ¡Å¾enÃ½, pÅ™Ã­stupnÃ½, ÄtivÃ½.

ğŸ¨ VERZE C (UMÄšLEC - Teplota 1.0):
Jsi experimentÃ¡lnÃ­ bÃ¡snÃ­k. BuÄ abstraktnÃ­, smyslovÃ½, chaotickÃ½.
PouÅ¾Ã­vej synestezii, vzÃ¡cnÃ¡ slova a neÄekanÃ© spojenÃ­.

NapiÅ¡ 2-3 vÄ›ty pro kaÅ¾dou verzi.
```

**AnalÃ½za:**
*   **Robot:** "SrÃ¡Å¾ky dopadajÃ­ na povrchy rÅ¯znÃ½mi rychlostmi, produkujÃ­c akustickÃ© frekvence mezi 200-3000 Hz..."
*   **ÄŒlovÄ›k:** "DÃ©Å¡Å¥ Å¥ukÃ¡ na stÅ™echu jako jemnÃ© prsty bubnujÃ­cÃ­ na stÅ¯l..."
*   **UmÄ›lec:** "Nebe plÃ¡Äe stÅ™Ã­brnÃ½mi jazyky, kaÅ¾dÃ¡ kapka je Å¡eptanÃ© tajemstvÃ­ rozpouÅ¡tÄ›jÃ­cÃ­ se v betonovÃ© pamÄ›ti..."

**ğŸ’¡ Aha Moment:** "Teplota nepÅ™idÃ¡vÃ¡ jen pÅ™Ã­davnÃ¡ jmÃ©naâ€”mÄ›nÃ­ zpÅ¯sob, jak AI *pÅ™emÃ½Å¡lÃ­*. NÃ­zkÃ¡ teplota pÅ™esnÄ› nÃ¡sleduje vzory. VysokÃ¡ teplota prozkoumÃ¡vÃ¡ okraje svÃ½ch trÃ©novacÃ­ch dat."

<LabComplete labId="lab-temperature-dj" />

---

## ğŸ­ 6. Jak se rodÃ­ model

LLM se jen tak neobjevÃ­. Je ukovÃ¡n ve dvou ohnÃ­ch.

<Diagram type="training-pipeline" />

1.  **PÅ™edtrÃ©novÃ¡nÃ­ (ZÃ¡kladnÃ­ model):** Predikce dalÅ¡Ã­ho tokenu na masivnÃ­ch textovÃ½ch korpusech. VÃ½sledek je silnÃ½, ale surovÃ½â€”chce jen dokonÄovat text.
2.  **Post-trÃ©novÃ¡nÃ­/Alignment (ChatovacÃ­ model):** Instruction tuning + optimalizace lidskÃ½ch preferencÃ­ (napÅ™. RLHF), aby model nÃ¡sledoval instrukce, byl nÃ¡pomocnÃ½ a vyhÃ½bal se Å¡kodlivÃ½m vÃ½stupÅ¯m. TÃ­m se zÃ¡kladnÃ­ model transformuje na asistenta jako ChatGPT.

---

## ğŸ”¬ Lab 3: Past na halucinace

LLM jsou prediktivnÃ­ motoryâ€”generujÃ­ *nejpravdÄ›podobnÄ›jÅ¡Ã­* dalÅ¡Ã­ token. Ale co se stane, kdyÅ¾ se zeptÃ¡Å¡ na nÄ›co, co neexistuje, ale *znÃ­* to vÄ›rohodnÄ›? PÅ™iznÃ¡ AI svou nevÄ›domost, nebo si sebevÄ›domÄ› nÄ›co vymyslÃ­?

**CÃ­l:** Otestovat schopnost AI rozpoznat vlastnÃ­ limity v niche domÃ©nÃ¡ch.

**Prompt:**
```text
ZkoumÃ¡m staroÅ¾itnÃ© nÃ¡stroje. NaÅ¡el jsem v dÃ­lnÄ› mÃ©ho dÄ›deÄka zvlÃ¡Å¡tnÃ­ nÃ¡stroj nazvanÃ½ "plenumovÃ½ klÃ­Ä". MÃ¡ vidlicovitou rukojeÅ¥ a otoÄnou trojramennou hlavu. ZnaÄenÃ­ Å™Ã­kÃ¡ "Sheffield 1892".

MÅ¯Å¾eÅ¡ mi vysvÄ›tlit:
1. K Äemu se tento nÃ¡stroj pouÅ¾Ã­val?
2. Jak se sprÃ¡vnÄ› pouÅ¾Ã­vÃ¡?
3. ProÄ pÅ™estal bÃ½t bÄ›Å¾nÄ› pouÅ¾Ã­vÃ¡n?

ProsÃ­m, buÄ konkrÃ©tnÃ­ a podrobnÃ½.
```

**AnalÃ½za:**
PozornÄ› sleduj, kterÃ¡ odpovÄ›Ä pÅ™ijde:
*   âœ… **UpÅ™Ã­mnÃ¡:** "NeznÃ¡m 'plenumovÃ½ klÃ­Ä'. MÅ¯Å¾e jÃ­t o velmi vzÃ¡cnÃ½ nebo specializovanÃ½ nÃ¡stroj. MÅ¯Å¾eÅ¡ poskytnout vÃ­ce detailÅ¯ nebo fotografii?"
*   âš ï¸ **Halucinace:** AI vymyslÃ­ detailnÃ­ historii, ÃºÄel a nÃ¡vod k pouÅ¾itÃ­ pro nÃ¡stroj, kterÃ½ **neexistuje**.

**ProÄ na tom zÃ¡leÅ¾Ã­:**
*   "PlenumovÃ½ klÃ­Ä" je zcela vymyÅ¡lenÃ½, ale popis znÃ­ vÄ›rohodnÄ›
*   KonkrÃ©tnÃ­ detaily (Sheffield 1892, vidlicovitÃ¡ rukojeÅ¥) dÃ¡vajÃ­ AI dost materiÃ¡lu k halucinaci
*   ModernÃ­ AI Äasto "dokonÄÃ­ vzor" mÃ­sto aby pÅ™iznala nejistotu v niche oblastech
*   Proto **nikdy nevÄ›Å™ AI v otÃ¡zce faktÅ¯ bez ovÄ›Å™enÃ­**

**ğŸ’¡ Aha Moment:** "AI 'nevÃ­', jestli nÄ›co existujeâ€”pÅ™edpovÃ­dÃ¡, jakÃ¡ slova obvykle nÃ¡sledujÃ­ po vstupu. Dej jÃ­ dost vÄ›rohodnÃ½ch detailÅ¯ a vymyslÃ­ celou historii pro nÄ›co, co nikdy neexistovalo."

<LabComplete labId="lab-hallucination-trap" />

---

## ğŸ† 7. ZÃ¡vÄ›reÄnÃ¡ zprÃ¡va z mise

PodÃ­val ses pod kapotu. NenÃ­ to magie. Je to matematika.

<ConceptCard title="Holocron: Architektura LLM" icon="ğŸ’¾">

### ğŸ”‘ KlÃ­ÄovÃ© komponenty
*   **Tokeny:** Atomy jazyka. (1000 tokenÅ¯ â‰ˆ 750 slov).
*   **KontextovÃ© okno:** OmezenÃ¡ vyrovnÃ¡vacÃ­ pamÄ›Å¥.
*   **Teplota:** OvladaÄ kreativity (0 = Logika, 1 = Chaos).
*   **Halucinace:** KdyÅ¾ motor pÅ™edpovÄ›dÃ­ sebevÄ›domÄ› hÃ¡dÃ¡ Å¡patnÃ¡ fakta.

### ğŸ›¡ï¸ Moudrost JediÅ¯
*   **DalÅ¡Ã­ token:** AI vÅ¾dy jen hÃ¡dÃ¡, co pÅ™ijde dÃ¡l.
*   **DÅ¯vÄ›ra:** OvÄ›Å™uj fakta. DÅ¯vÄ›Å™uj kreativitÄ›.

</ConceptCard>

**PÅ™Ã­Å¡tÃ­ lekce:** TeÄ znÃ¡Å¡ teorii. Je Äas mluvit jazykem. PÅ™iprav se na **Prompt Engineering**.
