# Jak se AI uÄÃ­?

<Callout type="info">
**CÃ­l mise:** ZvlÃ¡dnout 3 zpÅ¯soby, jak se stroje uÄÃ­, zÃ¡hadu "ÄŒernÃ© skÅ™Ã­Åˆky" a jak je modernÃ­ LLM kombinujÃ­.
â³ **ÄŒas ÄtenÃ­:** 20 min | ğŸ§ª **[3] Laby souÄÃ¡stÃ­**
</Callout>

<VideoSwitcher alternatives={[{"id":"Ilg3gGewQ5U","title":"Backpropagation (3B1B)"},{"id":"aircAruvnKk","title":"NeuronovÃ© sÃ­tÄ› (3B1B Klasika)"},{"id":"E0Hmnixke2g","title":"VÅ¡echny ML algoritmy za 17 min (2024)"},{"id":"zQwL4uo3kf0","title":"Jak funguje AI? (CZ 2025)"}]} />

## âš¡ ZjevenÃ­

**Neuron je jen ÄÃ­slo.**

To je celÃ©. Å½Ã¡dnÃ¡ magie. Å½Ã¡dnÃ© vÄ›domÃ­. Jen ÄÃ­slo uloÅ¾enÃ© v poÄÃ­taÄi. A pÅ™esto, kdyÅ¾ propojÃ­Å¡ miliardy tÄ›chto ÄÃ­sel sprÃ¡vnÃ½m zpÅ¯sobem... dostaneÅ¡ modernÃ­ AI jako ChatGPT.

Jak mÅ¯Å¾e pÅ™idÃ¡nÃ­ dalÅ¡Ã­ch ÄÃ­sel vytvoÅ™it *porozumÄ›nÃ­*? Jak se hromada matematiky nauÄÃ­ psÃ¡t poezii, programovat software a diagnostikovat nemoci?

**PrÃ¡vÄ› se to dozvÃ­Å¡.**

PodÃ­vej se na video vÃ½Å¡eâ€”je to nejkrÃ¡snÄ›jÅ¡Ã­ vysvÄ›tlenÃ­ neuronovÃ½ch sÃ­tÃ­, jakÃ© kdy vzniklo. Vizualizace od 3Blue1Brown navÅ¾dy zmÄ›nÃ­ zpÅ¯sob, jakÃ½m vidÃ­Å¡ AI.

---

## ğŸ§  1. UvnitÅ™ stroje (MNIST ZjevenÃ­)

Video ukazuje nÄ›co hlubokÃ©ho: AI se uÄÃ­ rozpoznÃ¡vat ruÄnÄ› psanÃ© ÄÃ­slice.

**NastavenÃ­:**
- Vstup: 784 pixelÅ¯ (obrÃ¡zek 28Ã—28 ruÄnÄ› psanÃ©ho ÄÃ­sla)
- VÃ½stup: KterÃ¡ ÄÃ­slice to je? (0-9)
- StÅ™ed: TisÃ­ce neuronÅ¯, kaÅ¾dÃ½ drÅ¾Ã­ jen... ÄÃ­slo.

<Diagram type="neural-network" />

### ğŸ’¡ KlÃ­ÄovÃ½ vhled
KaÅ¾dÃ½ neuron se â€aktivuje" (vÃ½stupnÃ­ hodnota se zvÃ½Å¡Ã­), kdyÅ¾ detekuje specifickÃ½ vzor. NÄ›kterÃ© neurony se nauÄÃ­ detekovat **hrany**. JinÃ© detekujÃ­ **kÅ™ivky**. DalÅ¡Ã­ detekujÃ­ **smyÄku v devÃ­tce**.

Nikdo tyto vzory neprogramuje. SÃ­Å¥ je **objevuje sama**.

*Toto je uÄenÃ­.*

---

## ğŸ“ TÅ™i trÃ©ninkovÃ© metody

TeÄ, kdyÅ¾ vidÃ­me, CO neuronovÃ¡ sÃ­Å¥ je, pojÄme pochopit, JAK ji trÃ©nujeme. PÅ™edstav si je jako **TrÃ©ninkovÃ© metody JediÅ¯**.

<Diagram type="learning-types-overview" />

---

## ğŸ“š 2. UÄenÃ­ s uÄitelem (Supervised Learning - TÅ™Ã­da)

Toto je nejbÄ›Å¾nÄ›jÅ¡Ã­ metoda. Funguje jako pÅ™Ã­snÃ¡ Å¡kolnÃ­ tÅ™Ã­da.
**UÄitel (ÄŒlovÄ›k)** dÃ¡vÃ¡ **Studentovi (AI)** problÃ©m A ODPOVÄšÄ.

### Workflow ğŸ”„
1.  ğŸ“¥ **Vstup:** ObrÃ¡zek koÄky ğŸ±
2.  ğŸ·ï¸ **Å tÃ­tek:** "Toto je KoÄka."
3.  ğŸ” **Opakovat:** 1 000 000 krÃ¡t.

AI se nakonec nauÄÃ­ vzory "KoÄky" (Å¡piÄatÃ© uÅ¡i, vousky) a dokÃ¡Å¾e rozpoznat koÄky, kterÃ© nikdy pÅ™edtÃ­m nevidÄ›la.

<Diagram type="supervised-learning-flow" />

<ConceptCard title="KuchaÅ™ka" icon="ğŸ“–" jediQuote="Studuj archivy.">
*   **Analogie:** KartiÄky s odpovÄ›Ämi na zadnÃ­ stranÄ›.
*   **PÅ™Ã­pad uÅ¾itÃ­:** Spam filtry, RozpoznÃ¡vÃ¡nÃ­ obliÄeje, LÃ©kaÅ™skÃ¡ diagnÃ³za.
*   **OmezenÃ­:** VyÅ¾aduje obrovskÃ© mnoÅ¾stvÃ­ lidmi oznaÄenÃ½ch dat (drahÃ©).
</ConceptCard>

### ğŸ”¬ Lab 1: UÄitel vzorÅ¯

PodÃ­vejme se, jak AI dokÃ¡Å¾e odvodit pravidla z pÅ™Ã­kladÅ¯. Toto je **In-Context Learning** â€” AI pouÅ¾ije pÅ™Ã­klady v tvÃ©m promptu k odvozenÃ­ vzoru (nejednÃ¡ se o trÃ©novÃ¡nÃ­, ale o chytrou inferenci).

**CÃ­l:** Objevit, Å¾e AI dokÃ¡Å¾e odvodit pravidla ÄistÄ› z pÅ™Ã­kladÅ¯ v promptu.

**Prompt:**
```text
PodÃ­vej se na tyto pÅ™Ã­klady a odhal pravidlo:

âœ… "Pes" â†’ "P"
âœ… "KoÄka" â†’ "K"
âœ… "Slon" â†’ "S"
âœ… "PapouÅ¡ek" â†’ "P"

TeÄ pouÅ¾ij pravidlo:
â“ "Å½irafa" â†’ ?
â“ "Tygr" â†’ ?
â“ "Ptakopysk" â†’ ?

Nakonec: PopiÅ¡ pravidlo, kterÃ© jsi objevil/a.
```

**AnalÃ½za:**
*   AI by mÄ›la odpovÄ›dÄ›t: Å½, T, P (prvnÃ­ pÃ­smeno kaÅ¾dÃ©ho slova)
*   Kouzlo: Nikdy jsi AI pravidlo NEÅ˜EKL/A. Sama ho ODVODILA ze vzorÅ¯.
*   PoznÃ¡mka: Toto je **in-context learning** (inference), ne skuteÄnÃ½ trÃ©nink. AI se nauÄila vzory bÄ›hem trÃ©ninkuâ€”teÄ je aplikuje na tvÃ© pÅ™Ã­klady.

**ğŸ’¡ Aha Moment:** "AI nepotÅ™ebuje explicitnÃ­ instrukceâ€”rozpoznÃ¡vÃ¡ vzory v pÅ™Ã­kladech, kterÃ© jÃ­ poskytnu. Proto prompty s pÅ™Ã­klady fungujÃ­ tak dobÅ™e!"

<LabComplete labId="lab-pattern-teacher" />

---

## ğŸ§© 3. UÄenÃ­ bez uÄitele (Unsupervised Learning - Meditace)

Co kdyÅ¾ nemÃ¡me Å¡tÃ­tky? Co kdyÅ¾ na AI prostÄ› vysypeme hrubÃ¡ data a Å™ekneme "PÅ™eber si to"?
To je **UÄenÃ­ bez uÄitele**. AI medituje nad daty, dokud nenajde skrytÃ© struktury.

### Workflow ğŸ”„
1.  ğŸ“¦ **Vstup:** 1 000 000 zÃ¡znamÅ¯ o nÃ¡kupech zÃ¡kaznÃ­kÅ¯.
2.  âŒ **Å tÃ­tek:** Å½Ã¡dnÃ½.
3.  âœ¨ **VÃ½sledek:** AI je seskupÃ­ do "VÃ­kendovÃ½ch nakupujÃ­cÃ­ch", "LovcÅ¯ slev" a "VelkÃ½ch utrÃ¡ceÄÅ¯".

<Diagram type="clustering-visualization" />

<ConceptCard title="HledaÄ vzorÅ¯" icon="ğŸ”®" jediQuote="Naslouchej SÃ­le. Ta tÄ› vede.">
*   **Analogie:** OrganizovÃ¡nÃ­ nepoÅ™Ã¡dnÃ© knihovny bez znalosti systÃ©mu tÅ™Ã­dÄ›nÃ­.
*   **PÅ™Ã­pad uÅ¾itÃ­:** Segmentace zÃ¡kaznÃ­kÅ¯, DoporuÄovacÃ­ systÃ©my (Netflix), Detekce anomÃ¡liÃ­.
</ConceptCard>

### ğŸ”¬ Lab 2: AbstraktnÃ­ vzor

AI dokÃ¡Å¾e rozpoznat **jakÃ½koliv abstraktnÃ­ vzor**, i ve vymyÅ¡lenÃ©m jazyce. To dokazuje, Å¾e AI pracuje se **vzory**, ne s vÃ½znamyâ€”nepotÅ™ebuje "rozumÄ›t" datÅ¯m, aby naÅ¡la strukturu.

**CÃ­l:** Vymyslet si pravidlo transformace a sledovat, jak ho AI okamÅ¾itÄ› odvodÃ­.

<Callout type="warning">
**PoznÃ¡mka:** Tento lab demonstruje odvozovÃ¡nÃ­ vzorÅ¯, ne skuteÄnÃ© uÄenÃ­ bez uÄitele (kterÃ© by zahrnovalo shlukovÃ¡nÃ­ bez pÅ™Ã­kladÅ¯). SkuteÄnÃ© unsupervised learning probÃ­hÃ¡ bÄ›hem trÃ©ninku AI, ne v chat promptu.
</Callout>

**Prompt:**
```text
Zahrajeme si s vymyÅ¡lenÃ½m jazykem. V mÃ©m jazyce existuje jednoduchÃ½ systÃ©m pÅ™Ã­pon:

Pes â†’ Pesofon (pÅ™idej "-ofon")
BanÃ¡n â†’ BanÃ¡nofon (pÅ™idej "-ofon")
Vlak â†’ Vlakofon (pÅ™idej "-ofon")

TeÄ mi Å™ekni:
1. Jak se Å™ekne "POÄŒÃTAÄŒ" v mÃ©m jazyce?
2. Jak se Å™ekne "STROM" v mÃ©m jazyce?
3. Jak se Å™ekne "SEN" v mÃ©m jazyce?

Pro KAÅ½DÃ‰ slovo VYSVÄšTLI pravidlo, kterÃ© jsi pouÅ¾il/a.
```

**AnalÃ½za:**
*   AI by mÄ›la vygenerovat: PoÄÃ­taÄofon, Stromofon, Senofon
*   Sleduj vysvÄ›tlenÃ­ â€” AI popÃ­Å¡e pravidlo, kterÃ© odvodila
*   To dokazuje, Å¾e AI pracuje na Ãºrovni SYMBOLÅ®, ne konceptÅ¯

**ğŸ’¡ Aha Moment:** "AI nerozumÃ­ 'vÃ½znamu'. RozpoznÃ¡vÃ¡ VZORY a aplikuje je na novÃ¡ data. Je jÃ­ jedno, jestli je jazyk skuteÄnÃ½ nebo vymyÅ¡lenÃ½ â€” vzor je vzor."

<LabComplete labId="lab-space-language" />

---

## âš”ï¸ 4. UÄenÃ­ posilovÃ¡nÃ­m (Reinforcement Learning - BojovÃ½ vÃ½cvik)

Toto je nejpokroÄilejÅ¡Ã­ metoda. Takto uÄÃ­me AI hrÃ¡t hry (Å achy, Go) nebo chodit (Roboti).
Je to **Pokus a Omyl** na steroidech.

### Workflow ğŸ”„
1.  ğŸ¤– **Akce:** AI udÄ›lÃ¡ tah.
2.  ğŸ¯ **ZpÄ›tnÃ¡ vazba:** DÃ¡me jÃ­ body (+1 za vÃ½hru, -1 za prohru).
3.  ğŸ” **SmyÄka:** Hraje miliony her sama proti sobÄ›, aby maximalizovala body.

<Diagram type="reinforcement-learning-loop" />

<ConceptCard title="VÃ¡leÄnÃ­k" icon="âš”ï¸" sithQuote="SilnÃ­ pÅ™eÅ¾ijÃ­. SlabÃ­ zahynou.">
*   **Analogie:** CviÄenÃ­ psa pomocÃ­ pamlskÅ¯.
*   **PÅ™Ã­pad uÅ¾itÃ­:** SamoÅ™Ã­dÃ­cÃ­ auta, AlphaGo, Robotika.
*   **KlÃ­Ä:** PotÅ™ebuje simulaci, aby mohla bÄ›Å¾et miliony krÃ¡t.
</ConceptCard>

### ğŸ”¬ Lab 3: RLHF SimulÃ¡tor

Tento lab simuluje, jak se AI skuteÄnÄ› zlepÅ¡uje pomocÃ­ **lidskÃ© zpÄ›tnÃ© vazby** â€” jÃ¡dro RLHF (Reinforcement Learning from Human Feedback).

**CÃ­l:** ZaÅ¾Ã­t zpÄ›tnovazebnÃ­ smyÄku, kterÃ¡ dÄ›lÃ¡ AI asistenty uÅ¾iteÄnÃ½mi.

**Prompt:**
```text
Budeme simulovat RLHF (Reinforcement Learning from Human Feedback - UÄenÃ­ posilovÃ¡nÃ­m z lidskÃ© zpÄ›tnÃ© vazby).

KOLO 1:
NapiÅ¡ krÃ¡tkÃ© vysvÄ›tlenÃ­ fotosyntÃ©zy pro 10letÃ© dÃ­tÄ›. (2-3 vÄ›ty)

[PoÄkej na mou zpÄ›tnou vazbu pÅ™ed Kolem 2]
```

**Po odpovÄ›di AI dej zpÄ›tnou vazbu:**
```text
ZPÄšTNÃ VAZBA: To bylo pÅ™Ã­liÅ¡ odbornÃ©. 10letÃ© dÃ­tÄ› by nerozumÄ›lo "chlorofyl" nebo "glukÃ³za".
ODMÄšNA: 3/10

KOLO 2:
Zkus to znovu. PouÅ¾Ã­vej jen slova, kterÃ¡ by dÃ­tÄ› znalo. PÅ™irovnej to k nÄ›Äemu zÃ¡bavnÃ©mu jako vaÅ™enÃ­ nebo kouzla.
```

**Po Kole 2 dej finÃ¡lnÃ­ zpÄ›tnou vazbu:**
```text
ZPÄšTNÃ VAZBA: Mnohem lepÅ¡Ã­! PÅ™irovnÃ¡nÃ­ k vaÅ™enÃ­ fungovalo skvÄ›le.
ODMÄšNA: 9/10

TeÄ vysvÄ›tli: Co se zmÄ›nilo mezi Kolem 1 a Kolem 2? Jak moje zpÄ›tnÃ¡ vazba ovlivnila tvou odpovÄ›Ä?
```

**AnalÃ½za:**
*   Sleduj, jak AI pÅ™izpÅ¯sobuje odpovÄ›Ä na zÃ¡kladÄ› tvÃ© zpÄ›tnÃ© vazby
*   Toto **simuluje** koncept RLHFâ€”ve skuteÄnosti je RLHF offline trÃ©novacÃ­ proces, kde tisÃ­ce lidskÃ½ch hodnocenÃ­ slouÅ¾Ã­ k trÃ©novÃ¡nÃ­ reward modelu
*   TvÃ¡ zpÄ›tnÃ¡ vazba v chatu ve skuteÄnosti neaktualizuje vÃ¡hy AIâ€”ale ukazuje ti *princip*, jak byl ChatGPT alignovÃ¡n

<Callout type="info">
**Kontrola reality:** V tomto labu hrajeÅ¡ RLHF na neÄisto. SkuteÄnÃ½ RLHF proces probÃ­hÃ¡ **offline** bÄ›hem trÃ©ninku: lidÃ© hodnotÃ­ tisÃ­ce vÃ½stupÅ¯ â†’ trÃ©nuje se reward model â†’ AI je doladÄ›na k maximalizaci tÃ©to odmÄ›ny. TvÅ¯j chat model nemÄ›nÃ­â€”ten uÅ¾ je natrÃ©novanÃ½!
</Callout>

**ğŸ’¡ Aha Moment:** "TeÄ chÃ¡pu, jak byl ChatGPT trÃ©novÃ¡n! LidÃ© hodnotili odpovÄ›di jako uÅ¾iteÄnÃ© nebo Å¡kodlivÃ© a AI se nauÄila maximalizovat pozitivnÃ­ hodnocenÃ­â€”to vÅ¡e jeÅ¡tÄ› pÅ™edtÃ­m, neÅ¾ jsem si s nÃ­ vÅ¯bec psala!"

<LabComplete labId="lab-rlhf-simulator" />

---

## ğŸ“‰ 5. Jak se vlastnÄ› zlepÅ¡uje (Hra na pÅ™ihoÅ™Ã­vÃ¡)

MoÅ¾nÃ¡ si Å™Ã­kÃ¡Å¡: *"DobÅ™e, ale jak se ta AI skuteÄnÄ› zlepÅ¡uje?"*
HÃ¡dÃ¡ prostÄ› nÃ¡hodnÄ›, dokud se netrefÃ­?

VlastnÄ› ano. Ale s hÃ¡Äkem. Hraje hru na **"PÅ™ihoÅ™Ã­vÃ¡ & SamÃ¡ voda"**.

### TrÃ©ninkovÃ¡ smyÄka ğŸ”„
1.  **Odhad:** AI se podÃ­vÃ¡ na obrÃ¡zek psa a tipne "KoÄka" (50% jistota).
2.  **Chyba (ZtrÃ¡ta):** UÄitel Å™ekne "Å patnÄ›! To byl Pes."
3.  **Aktualizace:** AI vypoÄÃ­tÃ¡, *o kolik* se spletla ("ZtrÃ¡ta") a poÅ¡Å¥ouchne svÃ¡ vnitÅ™nÃ­ ÄÃ­sla (vÃ¡hy), aby se pÅ™Ã­Å¡tÄ› spletla o trochu mÃ©nÄ›.

Tento proces opakuje **miliony krÃ¡t**.
*   Odhad -> Å patnÄ› -> PoÅ¡Å¥ouchnutÃ­.
*   Odhad -> Å patnÄ› -> PoÅ¡Å¥ouchnutÃ­.
*   Odhad -> SprÃ¡vnÄ›! -> UloÅ¾it.

<Diagram type="training-loop" />

<ConceptCard title="ZtrÃ¡tovÃ¡ funkce" icon="ğŸ“‰" jediQuote="NeÃºspÄ›ch, nejvÄ›tÅ¡Ã­m uÄitelem je.">
*   **Analogie:** HranÃ­ "PÅ™ihoÅ™Ã­vÃ¡" se zavÃ¡zanÃ½ma oÄima.
*   **CÃ­l:** Minimalizovat "ZtrÃ¡tu" (vzdÃ¡lenost od sprÃ¡vnÃ© odpovÄ›di).
*   **Magie:** AI to dÄ›lÃ¡ pomocÃ­ kalkulu (GradientnÃ­ sestup), klouzÃ¡nÃ­m z kopce chyby dolÅ¯, dokud nenajde dno (Pravdu).
</ConceptCard>

---

## ğŸ“¦ 6. ProblÃ©m "ÄŒernÃ© skÅ™Ã­Åˆky"

Tady je ta dÄ›sivÃ¡ ÄÃ¡st. ZnÃ¡me **Vstup** (Data) a znÃ¡me **VÃ½stup** (PÅ™edpovÄ›Ä).
Ale ten stÅ™ed? Ta ÄÃ¡st, kde AI skuteÄnÄ› "pÅ™emÃ½Å¡lÃ­"?

Je to **ÄŒernÃ¡ skÅ™Ã­Åˆka**.

<Diagram type="black-box" />

UvnitÅ™ modernÃ­ AI (jako GPT-4) je odhadovanÄ› **stovky miliard** ÄÃ­sel (parametrÅ¯), kterÃ© interagujÃ­ sloÅ¾itÃ½mi zpÅ¯sobyâ€”pÅ™esnÃ½ poÄet nenÃ­ veÅ™ejnÄ› znÃ¡m.
KdyÅ¾ se zeptÃ¡Å¡ inÅ¾enÃ½ra *"ProÄ AI Å™ekla 'Ahoj' mÃ­sto 'ÄŒau'?"*, nemÅ¯Å¾e ukÃ¡zat na konkrÃ©tnÃ­ Å™Ã¡dek kÃ³du.
MÅ¯Å¾e jen Å™Ã­ct: *"ProtoÅ¾e matematika rozhodla, Å¾e to bylo o 0,001 % pravdÄ›podobnÄ›jÅ¡Ã­."*

### ProÄ na tom zÃ¡leÅ¾Ã­
*   **DÅ¯vÄ›ra:** MÅ¯Å¾eme vÄ›Å™it rozhodnutÃ­, kdyÅ¾ nerozumÃ­me dÅ¯vodÅ¯m?
*   **ZkreslenÃ­:** Pokud se AI nauÄÃ­ zlozvyk (jako rasismus) z internetu, je tÄ›Å¾kÃ© najÃ­t, kde pÅ™esnÄ› je ten zvyk uloÅ¾en, aby se smazal.
*   **Magie:** TakÃ© to znamenÃ¡, Å¾e AI mÅ¯Å¾e najÃ­t vzory, kterÃ© lidÃ© *nevidÃ­*, a Å™eÅ¡it problÃ©my zpÅ¯soby, kterÃ© jsme si nikdy nepÅ™edstavovali.

---

## ğŸ¤ 7. TajnÃ¡ zbraÅˆ: RLHF

Jak jsme dostali ChatGPT? Nebyla to jen jedna metoda. Byla to kombinace.
Tou tajnou pÅ™Ã­sadou je **RLHF (Reinforcement Learning from Human Feedback - UÄenÃ­ posilovÃ¡nÃ­m z lidskÃ© zpÄ›tnÃ© vazby)**.

1.  ğŸ“š **Self-Supervised Pretraining:** NauÄila se jazykovÃ© vzory pÅ™edpovÃ­dÃ¡nÃ­m dalÅ¡Ã­ho slova na miliardÃ¡ch webovÃ½ch strÃ¡nekâ€”bez lidskÃ½ch Å¡tÃ­tkÅ¯.
2.  ğŸ‘ **RLHF (Reinforcement):** LidÃ© ohodnotili mnoho odpovÄ›dÃ­ a natrÃ©novali â€reward model", kterÃ½ pÅ™edpovÃ­dÃ¡, co lidÃ© preferujÃ­. AI pak byla doladÄ›na, aby maximalizovala tuto odmÄ›nu.
    *   NÃ¡pomocnÃ©, upÅ™Ã­mnÃ© odpovÄ›di â†’ VysokÃ¡ odmÄ›na.
    *   HrubÃ© nebo Å¡kodlivÃ© odpovÄ›di â†’ NÃ­zkÃ¡ odmÄ›na.

Tento alignment proces pÅ™etvoÅ™il surovÃ½ jazykovÃ½ model v uÅ¾iteÄnÃ©ho asistenta.

---

## ğŸ† 8. ZÃ¡vÄ›reÄnÃ¡ zprÃ¡va z mise

NynÃ­ znÃ¡Å¡ tÅ™i pilÃ­Å™e uÄenÃ­ AI.

<ConceptCard title="Holocron: Metody uÄenÃ­" icon="ğŸ§ ">

### 1. Supervised Learning (UÄitel) ğŸ“–
*   **Data:** OznaÄenÃ¡ (OtÃ¡zka + OdpovÄ›Ä).
*   **CÃ­l:** PÅ™esnost.
*   **PÅ™Ã­klad:** Je tento email Spam?

### 2. Unsupervised Learning (PrÅ¯zkumnÃ­k) ğŸ§­
*   **Data:** NeoznaÄenÃ¡ (HrubÃ¡ data).
*   **CÃ­l:** Struktura a Vzory.
*   **PÅ™Ã­klad:** DoporuÄenÃ­ Spotify.

### 3. Reinforcement Learning (HrÃ¡Ä) ğŸ®
*   **Data:** ProstÅ™edÃ­ (Akce + OdmÄ›ny).
*   **CÃ­l:** Strategie a Optimalizace.
*   **PÅ™Ã­klad:** KrÃ¡ÄejÃ­cÃ­ robot.

</ConceptCard>

**PÅ™Ã­Å¡tÃ­ lekce:** TeÄ, kdyÅ¾ mÃ¡me mozek, pojÄme mu dÃ¡t hlas. Vstupujeme do svÄ›ta **VelkÃ½ch jazykovÃ½ch modelÅ¯ (LLMs)**.
