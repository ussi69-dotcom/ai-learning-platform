<Callout type="info">
**Mise:** OvlÃ¡dnÄ›te umÄ›nÃ­ provozovÃ¡nÃ­ nejmodernÄ›jÅ¡Ã­ch (state-of-the-art) LLM na vlastnÃ­m hardwaru. OddÄ›lÃ­te svÃ© AI workflow od zÃ¡vislosti na cloudu, ÄÃ­mÅ¾ zajistÃ­te maximÃ¡lnÃ­ soukromÃ­, nulovou latenci a fixnÃ­ nÃ¡klady. Na konci tÃ©to lekce budete na svÃ©m lokÃ¡lnÃ­m stroji provozovat Llama 4 8B a integrovat ji do Python aplikace.

â³ **ÄŒas ÄtenÃ­:** 30 min | ğŸ§ª **[2] Laby souÄÃ¡stÃ­**
</Callout>

## ğŸ¥ DoporuÄenÃ© sledovÃ¡nÃ­

<Callout type="tip">
AÄkoliv je ÄtenÃ­ zÃ¡sadnÃ­, vidÄ›t lokÃ¡lnÃ­ inferenci v akci pomÃ¡hÃ¡ upevnit koncepty. PodÃ­vejte se na tento pÅ™ehled prostÅ™edÃ­ lokÃ¡lnÃ­ch LLM (edice Prosinec 2025).
*(Video Placeholder: https://youtube.com/watch?v=placeholder-local-llm-guide)*
</Callout>

## ğŸ° ProÄ spouÅ¡tÄ›t modely lokÃ¡lnÄ›?

V Ã©Å™e, kterÃ© dominujÃ­ masivnÃ­ poskytovatelÃ© API jako OpenAI a Anthropic, hnutÃ­ smÄ›rem k **LokÃ¡lnÃ­ Inteligenci**â€”spouÅ¡tÄ›nÃ­ modelÅ¯ na vlastnÃ­m hardwaru (nebo privÃ¡tnÃ­ch VPC)â€”explodovalo. ProÄ se obtÄ›Å¾ovat sprÃ¡vou vlastnÃ­ infrastruktury, kdyÅ¾ jsou API tak pohodlnÃ¡?

ExistujÃ­ ÄtyÅ™i kritickÃ© faktory:

### 1. SoukromÃ­ a datovÃ¡ suverenita
Toto je "killer feature" pro podnikovÃ© i osobnÃ­ pouÅ¾itÃ­. KdyÅ¾ pouÅ¾Ã­vÃ¡te API, posÃ­lÃ¡te svÃ¡ data (prompty, kontext, RAG dokumenty) tÅ™etÃ­ stranÄ›. I s politikami "nulovÃ©ho uchovÃ¡vÃ¡nÃ­" (zero retention) data opouÅ¡tÄ›jÃ­ vÃ¡Å¡ perimetr.
*   **LokÃ¡lnÄ›:** VaÅ¡e data nikdy neopustÃ­ vaÅ¡i RAM. MÅ¯Å¾ete je krmit PII, lÃ©kaÅ™skÃ½mi zÃ¡znamy nebo proprietÃ¡rnÃ­m kÃ³dem s nulovÃ½m rizikem Ãºniku nebo scrapingu pro trÃ©novÃ¡nÃ­.
*   **Air-gapped:** Tyto modely mÅ¯Å¾ete provozovat na strojÃ­ch, kterÃ© jsou zcela bez pÅ™ipojenÃ­ k internetu.

### 2. PÅ™edvÃ­datelnost nÃ¡kladÅ¯
NÃ¡klady na API rostou lineÃ¡rnÄ› s pouÅ¾Ã­vÃ¡nÃ­m. VirÃ¡lnÃ­ agentnÃ­ workflow mÅ¯Å¾e dnes stÃ¡t $5 a zÃ­tra $500.
*   **LokÃ¡lnÄ›:** NÃ¡klady jsou fixnÃ­ (hardware) plus elektÅ™ina. Jakmile mÃ¡te GPU, spuÅ¡tÄ›nÃ­ 1 milionu tokenÅ¯ stojÃ­ stejnÄ› jako spuÅ¡tÄ›nÃ­ 10. To je klÃ­ÄovÃ© pro "AgentnÃ­ smyÄky" (Agentic Loops), kde modely mohou pÅ™emÃ½Å¡let, kritizovat a opakovat pokusy stokrÃ¡t pro jedinÃ½ Ãºkol.

### 3. Latence
SÃ­Å¥ovÃ© round-tripy pÅ™idÃ¡vajÃ­ latenci.
*   **LokÃ¡lnÄ›:** Jste omezeni pouze propustnostÃ­ pamÄ›ti. Na Mac Studio (M4 Ultra) nebo NVIDIA RTX 5090 je "Time to First Token" (TTFT) Äasto rychlejÅ¡Ã­, neÅ¾ API call stihne provÃ©st handshake.

### 4. NecenzurovanÃ¡ a specializovanÃ¡ kontrola
API modely jsou silnÄ› upraveny pomocÃ­ RLHF (Reinforced Learning with Human Feedback) pro bezpeÄnost, coÅ¾ Äasto vede k chovÃ¡nÃ­ "odmÃ­tnutÃ­" (refusal) u neÅ¡kodnÃ½ch, ale hraniÄnÃ­ch ÃºkolÅ¯.
*   **LokÃ¡lnÄ›:** MÅ¯Å¾ete spouÅ¡tÄ›t "abliterated" nebo fine-tuned verze modelÅ¯, kterÃ© dÄ›lajÃ­ pÅ™esnÄ› to, co jim Å™eknete, bez moralizujÃ­cÃ­ch kÃ¡zÃ¡nÃ­.

---

## ğŸ“‰ VysvÄ›tlenÃ­ kvantizace

Pro spuÅ¡tÄ›nÃ­ modelu musÃ­te naÄÃ­st jeho **VÃ¡hy** (Weights) do VRAM (Video RAM). StandardnÃ­ trÃ©nink probÃ­hÃ¡ v **FP16** (16-bit Floating Point) nebo **BF16**.

*   **FP16:** KaÅ¾dÃ½ parametr zabÃ­rÃ¡ **2 byty**.
*   **Llama 4 8B @ FP16:** 8 000 000 000 Ã— 2 byty = 16 GB.

Pro mnoho spotÅ™ebitelskÃ½ch karet s 8GB nebo 12GB VRAM je to pÅ™Ã­liÅ¡ mnoho. PÅ™ichÃ¡zÃ­ **Kvantizace**.

Kvantizace sniÅ¾uje pÅ™esnost vah z 16bitovÃ½ch floatÅ¯ na lower-bit integery (Int8, Int4, atd.).

*   **Q8 (8-bit):** TÃ©mÄ›Å™ bezztrÃ¡tovÃ©. 1 byte na parametr (~8GB pro 8B model).
*   **Q4_K_M (4-bit):** Tzv. "Sweet Spot". Zhruba 0,5 bytu na parametr.
*   **Q2 (2-bit):** VÃ½znamnÃ© poÅ¡kozenÃ­ mozku (model se stÃ¡vÃ¡ nesouvislÃ½m).

**PoÅ¡kozuje to inteligenci?**
PÅ™ekvapivÄ›, modernÃ­ techniky kvantizace (jako GGUF/llama.cpp k-quants) ukazujÃ­, Å¾e **Q4** si zachovÃ¡vÃ¡ asi **95-98 %** schopnostÃ­ usuzovÃ¡nÃ­ modelu ve srovnÃ¡nÃ­ s FP16, zatÃ­mco vyÅ¾aduje o **60-70 % mÃ©nÄ› pamÄ›ti**.

<ConceptCard title="VÃ¡lka formÃ¡tÅ¯: GGUF vs Safetensors">
**Safetensors:** Standard pro surovÃ© vÃ¡hy (Hugging Face default). RychlÃ© naÄÃ­tÃ¡nÃ­, bezpeÄnÃ© (Å¾Ã¡dnÃ© spouÅ¡tÄ›nÃ­ pickle), ale obvykle vyÅ¾aduje konverzi pro specifickÃ© enginy.

**GGUF (GPT-Generated Unified Format):** KrÃ¡l lokÃ¡lnÃ­ inference. NavrÅ¾en pro `llama.cpp`. BalÃ­ vÃ¡hy modelu *a* informace o tokenizÃ©ru do jedinÃ©ho souboru. Podporuje **memory mapping**, coÅ¾ znamenÃ¡, Å¾e mÅ¯Å¾ete naÄÃ­st model vÄ›tÅ¡Ã­ neÅ¾ vaÅ¡e RAM odloÅ¾enÃ­m ÄÃ¡stÃ­ na disk (i kdyÅ¾ je to pomalÃ©).
</ConceptCard>

---

## ğŸ§® Matematika VRAM: SpustÃ­m to?

PÅ™ed staÅ¾enÃ­m modelu si musÃ­te spoÄÃ­tat, zda se vÃ¡m vejde do pamÄ›ti. Pokud pÅ™ekroÄÃ­te kapacitu VRAM, vrstvy modelu se pÅ™esunou do systÃ©movÃ© RAM (CPU), coÅ¾ zpomalÃ­ generovÃ¡nÃ­ ze 100 tokenÅ¯/s na 2 tokeny/s.

### VzoreÄek

```
CelkovÃ¡ VRAM â‰ˆ (Parametry Ã— BajtyNaParametr) + KV Cache + ReÅ¾ie aktivace
```

**ObecnÃ© pravidlo (Q4_K_M):**
PoÄÃ­tejte s **0,7 GB** na **1 miliardu** parametrÅ¯, plus **2-4 GB rezervy** pro kontextovÃ© okno (KV Cache).

*   **8B Model:** 8 Ã— 0,7 = 5,6 GB. Vejde se pohodlnÄ› na 8GB kartu.
*   **70B Model:** 70 Ã— 0,7 = 49 GB. VyÅ¾aduje dvÄ› 24GB karty nebo Mac s 64GB+ Unified Memory.

### PÅ™ehled nÃ¡rokÅ¯ na VRAM (Prosinec 2025)

| Kategorie | Model | VRAM (Q4_K_M) | PouÅ¾itÃ­ |
|----------|-------|---------------|----------|
| **Tiny** | Gemma 3 2B | < 2 GB | CPU inference, IoT |
| **Standard** | Llama 4 8B | ~ 6 GB | KaÅ¾dodennÃ­ pouÅ¾itÃ­ |
| **Mid** | Qwen 3 14B | ~ 10 GB | KrÃ¡l logiky |
| **Reasoning**| DeepSeek-R1 | ~ 12 GB | Matematika, logika |
| **Large** | Llama 4 70B | ~ 40 GB | ProdukÄnÃ­ kvalita |

<Callout type="warning">
**Dopad kontextovÃ©ho okna:** PoÅ¾adavky na VRAM pÅ™edpoklÃ¡dajÃ­ malÃ½ kontext (4k-8k). ZaplnÄ›nÃ­ 128k kontextu mÅ¯Å¾e spotÅ™ebovat desÃ­tky GB v KV Cache!
</Callout>

---

## ğŸï¸ InferenÄnÃ­ enginy

Jak vlastnÄ› spouÅ¡tÃ­te tyto GGUF nebo Safetensor soubory?

### 1. Ollama
*   **NejlepÅ¡Ã­ pro:** VÃ½vojÃ¡Å™e, zaÄÃ¡teÄnÃ­ky, uÅ¾ivatele Mac/Linux.
*   **"Docker" pro AI:** Abstrahuje vÅ¡e za jednoduchÃ© CLI. `ollama run llama4`.
*   **Backend:** Postaven na `llama.cpp`.
*   **API:** Poskytuje lokÃ¡lnÃ­ API kompatibilnÃ­ se specifikacÃ­ OpenAI.

### 2. LM Studio
*   **NejlepÅ¡Ã­ pro:** VizuÃ¡lnÃ­ typy, objevovÃ¡nÃ­ modelÅ¯.
*   **UI:** VyleÅ¡tÄ›nÃ¡ desktopovÃ¡ aplikace pro prohledÃ¡vÃ¡nÃ­ HuggingFace, stahovÃ¡nÃ­ GGUF a chatovÃ¡nÃ­.
*   **Funkce:** SkvÄ›lÃ½ vizualizÃ©r, kolik VRAM model zabere, neÅ¾ ho naÄtete.

### 3. vLLM
*   **NejlepÅ¡Ã­ pro:** Produkci, vysokou propustnost (High Throughput).
*   **Tech:** PagedAttention. Optimalizuje pamÄ›Å¥ specificky pro obsluhu soubÄ›Å¾nÃ½ch poÅ¾adavkÅ¯.
*   **PouÅ¾itÃ­:** Pokud stavÃ­te backend pro obsluhu 100 uÅ¾ivatelÅ¯ souÄasnÄ›, pouÅ¾ijte vLLM, ne Ollama.

---

## ğŸ§ª Lab 1: ZprovoznÄ›nÃ­ Ollama

### ğŸ¯ CÃ­l
Nainstalujte runtime Ollama a proveÄte svou prvnÃ­ lokÃ¡lnÃ­ inferenci pomocÃ­ modelu Llama 4 8B.

### ğŸ“‹ Prerekvizity
- Linux, macOS nebo Windows (doporuÄeno WSL2).
- AlespoÅˆ 8 GB RAM (doporuÄeno 16 GB).
- VolitelnÃ©, ale doporuÄenÃ©: NVIDIA GPU nebo Apple Silicon (M1/M2/M3/M4).

### ğŸ› ï¸ Kroky

#### 1. Instalace Ollama
OtevÅ™ete terminÃ¡l.

**Linux / macOS:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**Windows:**
StÃ¡hnÄ›te si spustitelnÃ½ soubor z [ollama.com](https://ollama.com).

#### 2. OvÄ›Å™enÃ­ instalace
```bash
ollama --version
# PÅ™Ã­klad vÃ½stupu: ollama version 0.5.12
```

#### 3. StaÅ¾enÃ­ a spuÅ¡tÄ›nÃ­ Llama 4
```bash
ollama run llama4:8b
```

*PoznÃ¡mka: TÃ­mto se stÃ¡hne pÅ™ibliÅ¾nÄ› 4,7 GB dat. PrvnÃ­ spuÅ¡tÄ›nÃ­ trvÃ¡ dÃ©le.*

#### 4. ChatovÃ¡ interakce
Jakmile se objevÃ­ prompt (`>>>`), zkuste tyto dotazy:

1.  **KreativnÃ­:** "NapiÅ¡ haiku o robotovi, kterÃ½ se uÄÃ­ milovat."
2.  **Logika:** "KdyÅ¾ mÃ¡m 3 jablka, jedno snÃ­m a pak koupÃ­m dvÄ› dalÅ¡Ã­, kolik jich mÃ¡m?"
3.  **KÃ³dovÃ¡nÃ­:** "NapiÅ¡ funkci v Pythonu pro obrÃ¡cenÃ­ Å™etÄ›zce."

PouÅ¾ijte `/bye` pro ukonÄenÃ­ chatu.

#### 5. Kontrola vyuÅ¾itÃ­ zdrojÅ¯
ZatÃ­mco model generuje text, otevÅ™ete novÃ½ terminÃ¡l pro sledovÃ¡nÃ­ vyuÅ¾itÃ­:

**Linux (NVIDIA):**
```bash
watch -n 1 nvidia-smi
```
*Sledujte VRAM usage (Memory-Usage) a GPU-Util.*

**macOS:**
OtevÅ™ete `Activity Monitor` -> `GPU History` (Cmd+4).

### âœ… KritÃ©ria ÃºspÄ›chu
- ÃšspÄ›Å¡nÄ› jste vstoupili do interaktivnÃ­ho reÅ¾imu chatu.
- OdpovÄ›di jsou generovÃ¡ny lokÃ¡lnÄ› (odpojte wifi pro dÅ¯kaz!).
- Zaznamenali jste nÃ¡rÅ¯st vyuÅ¾itÃ­ pamÄ›ti ve vaÅ¡em systÃ©mu.

---

## ğŸ§ª Lab 2: Integrace Pythonu pÅ™es API

### ğŸ¯ CÃ­l
OvlÃ¡dejte svÅ¯j lokÃ¡lnÃ­ LLM programovÄ› pomocÃ­ Pythonu. Toto je zÃ¡klad pro budovÃ¡nÃ­ agentÅ¯.

### ğŸ“‹ Prerekvizity
- NainstalovanÃ½ Python 3.10+.
- Ollama bÄ›Å¾Ã­cÃ­ na pozadÃ­ (`ollama serve` nebo mÃ­t aplikaci otevÅ™enou).

### ğŸ› ï¸ Kroky

#### 1. NastavenÃ­ prostÅ™edÃ­
VytvoÅ™te sloÅ¾ku pro projekt a nainstalujte oficiÃ¡lnÃ­ knihovnu.

```bash
mkdir local-agent
cd local-agent
python3 -m venv venv
source venv/bin/activate  # Na Windows: venv\Scripts\activate
pip install ollama
```

#### 2. VytvoÅ™enÃ­ inferenÄnÃ­ho skriptu
VytvoÅ™te soubor s nÃ¡zvem `agent.py`.

```python
import ollama
import time

# Definujeme model, kterÃ½ jsme stÃ¡hli v Lab 1
MODEL = "llama4:8b"

def chat_with_local_model(prompt):
    print(f"\nğŸ¤– OdesÃ­lÃ¡m do {MODEL}...\n")

    start_time = time.time()

    # Volba 'stream=True' umoÅ¾Åˆuje vidÄ›t tokeny tak, jak se generujÃ­
    # stejnÄ› jako v ChatGPT UI
    stream = ollama.chat(
        model=MODEL,
        messages=[{'role': 'user', 'content': prompt}],
        stream=True,
    )

    full_response = ""
    for chunk in stream:
        content = chunk['message']['content']
        print(content, end='', flush=True)
        full_response += content

    end_time = time.time()
    duration = end_time - start_time

    # VÃ½poÄet tokenÅ¯ za sekundu (pÅ™ibliÅ¾nÄ›)
    # HrubÃ¡ heuristika: 1 slovo ~= 1.3 tokenu
    token_count = len(full_response.split()) * 1.3
    tps = token_count / duration

    print(f"\n\nâš¡ Statistiky: {tps:.2f} tokenÅ¯/s | ÄŒas: {duration:.2f}s")

if __name__ == "__main__":
    user_input = input("Zadejte prompt: ")
    chat_with_local_model(user_input)
```

#### 3. SpuÅ¡tÄ›nÃ­ skriptu
```bash
python agent.py
```

**TestovacÃ­ vstup:** "VysvÄ›tli koncept rekurze pÄ›tiletÃ©mu dÃ­tÄ›ti."

### ğŸ“ Å˜eÅ¡enÃ­ problÃ©mÅ¯
*   **OdmÃ­tnutÃ­ pÅ™ipojenÃ­:** UjistÄ›te se, Å¾e Ollama bÄ›Å¾Ã­. Ve vÃ½chozÃ­m nastavenÃ­ naslouchÃ¡ na `127.0.0.1:11434`.
*   **PomalÃ© generovÃ¡nÃ­:** Pokud vidÃ­te < 2 tokeny/s, model se pravdÄ›podobnÄ› neveÅ¡el do VRAM a bÄ›Å¾Ã­ na CPU. Zkuste menÅ¡Ã­ model jako `gemma3:2b`.

### âœ… KritÃ©ria ÃºspÄ›chu
- Skript se spustÃ­ bez chyb pÅ™ipojenÃ­.
- VÃ½stup se streamuje do konzole v reÃ¡lnÃ©m Äase.
- Na konci vidÃ­te souhrn statistik ukazujÃ­cÃ­ tokeny za sekundu (TPS).

---

## ğŸ”® Holocron: KlÃ­ÄovÃ© poznatky

<ConceptCard title="Architektura lokÃ¡lnÃ­ inteligence">
1.  **Modely jsou soubory:** Jsou to prostÄ› velkÃ© binÃ¡rnÃ­ soubory (GGUF/Safetensors) obsahujÃ­cÃ­ vÃ¡hy.
2.  **RAM je limit:** VaÅ¡e VRAM urÄuje, jak "chytrÃ½" model mÅ¯Å¾ete spustit. Kvantizace (Q4) umoÅ¾Åˆuje vejÃ­t vÄ›tÅ¡Ã­ modely do menÅ¡Ã­ RAM s minimÃ¡lnÃ­ ztrÃ¡tou kvality.
3.  **InferenÄnÃ­ enginy:** NÃ¡stroje jako Ollama fungujÃ­ jako "operaÄnÃ­ systÃ©m" pro tyto soubory, vystavujÃ­ je pÅ™es HTTP API pro vaÅ¡i aplikaci.
4.  **Kompromis:** VymÄ›Åˆujete nekoneÄnou Å¡kÃ¡lovatelnost cloudu za soukromÃ­, kontrolu nÃ¡kladÅ¯ a suverenitu lokÃ¡lnÃ­ho kÅ™emÃ­ku.
</ConceptCard>
