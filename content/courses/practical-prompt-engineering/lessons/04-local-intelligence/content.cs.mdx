# LokÃ¡lnÃ­ inteligence ğŸ–¥ï¸

<Callout type="info">
**Mise:** OvlÃ¡dnÄ›te umÄ›nÃ­ provozovÃ¡nÃ­ nejmodernÄ›jÅ¡Ã­ch LLM na vlastnÃ­m hardwaru. ZÃ­skÃ¡te maximÃ¡lnÃ­ soukromÃ­, nulovou latenci a fixnÃ­ nÃ¡klady.

â³ **Reading Time:** 40 min | ğŸ§ª **[3] Labs Included**

</Callout>

<VideoSwitcher videos={[{"id":"QVOy61pXdSo","title":"Why Run AI Locally","author":"David Ondrej","description":"Local-first philosophy and model selection.","lang":"en"}]} />

---

## ğŸ¥ DoporuÄenÃ© video

<Callout type="tip">
Toto video od NetworkChuck ukazuje kompletnÃ­ postup od nuly po funkÄnÃ­ lokÃ¡lnÃ­ AI.

ğŸ”— **EN:** [host ALL your AI locally](https://www.youtube.com/watch?v=Wjrdr0NU4Sk) â€” NetworkChuck (20 min)

ğŸ‡¨ğŸ‡¿ **CZ Alternativa:** [ChatGPT na vaÅ¡em poÄÃ­taÄi](https://www.youtube.com/watch?v=R_iA-S2N-iE) â€” AI s rozumem

</Callout>

---

## 1. ProÄ spouÅ¡tÄ›t modely lokÃ¡lnÄ›? ğŸ°

V Ã©Å™e dominance API jako OpenAI a Anthropic hnutÃ­ k **LokÃ¡lnÃ­ Inteligenci** explodovalo. ProÄ se obtÄ›Å¾ovat vlastnÃ­ infrastrukturou?

<Diagram type="local-llm-architecture" />

### 4 kritickÃ© dÅ¯vody

| DÅ¯vod           | Cloud API              | LokÃ¡lnÃ­ LLM                |
| --------------- | ---------------------- | -------------------------- |
| **ğŸ”’ SoukromÃ­** | Data opouÅ¡tÃ­ perimetr  | Data nikdy neopustÃ­ RAM    |
| **ğŸ’° NÃ¡klady**  | $0.01-0.06 / 1k tokenÅ¯ | FixnÃ­ (HW + elektÅ™ina)     |
| **âš¡ Latence**  | 150-400ms              | 10-20ms                    |
| **ğŸ›ï¸ Kontrola** | Censored, RLHF         | PlnÃ¡ kontrola, fine-tuning |

<Callout type="warning">
  **Reality Check 2025:** 1 milion tokenÅ¯ pÅ™es GPT-4o stojÃ­ ~$30. LokÃ¡lnÄ› na RTX
  4090 stojÃ­ ~$0.02 (elektÅ™ina). Pro agentnÃ­ smyÄky, kde model iteruje 100Ã—, je
  rozdÃ­l astronomickÃ½.
</Callout>

---

## 2. Benchmark Arena 2025 ğŸ“Š

Jak si lokÃ¡lnÃ­ modely vedou proti cloudovÃ½m gigantÅ¯m?

<Diagram type="model-benchmark-chart" />

### DetailnÃ­ srovnÃ¡nÃ­

| Model                | MMLU-Pro | VRAM (Q4) | Tokeny/s (RTX 5090) | PoznÃ¡mka             |
| -------------------- | -------- | --------- | ------------------- | -------------------- |
| **GPT-5.1**          | 92.1%    | â˜ï¸ Cloud  | N/A                 | Reference cloud      |
| **Claude Opus 4.5**  | 91.5%    | â˜ï¸ Cloud  | N/A                 | NejlepÅ¡Ã­ pro kÃ³d     |
| **Gemini 3 Pro**     | 91.0%    | â˜ï¸ Cloud  | N/A                 | 1M kontext           |
| **Llama 4 Maverick** | 89.2%    | ~45 GB    | 35 t/s              | NejlepÅ¡Ã­ open-source |
| **Qwen 3 235B**      | 88.5%    | ~90 GB    | 15 t/s              | Alibaba flagship     |
| **Llama 4 Scout**    | 80.5%    | ~8 GB     | 100 t/s             | Budget krÃ¡l          |
| **DeepSeek-R1**      | 78.3%    | ~12 GB    | 60 t/s              | Reasoning specialist |

<Callout type="tip">
  **Pro Tip:** **Llama 4 Scout** je sweet spot pro vÄ›tÅ¡inu pouÅ¾itÃ­. 80.5% na
  MMLU-Pro je lepÅ¡Ã­ neÅ¾ GPT-4 a bÄ›Å¾Ã­ na 8GB GPU.
</Callout>

---

## 3. Kvantizace: Magie komprese ğŸ“‰

Pro spuÅ¡tÄ›nÃ­ modelu musÃ­te naÄÃ­st jeho **vÃ¡hy** do VRAM. StandardnÃ­ trÃ©nink probÃ­hÃ¡ v **FP16** (16-bit).

### Matematika

```text
Llama 4 8B @ FP16:
8,000,000,000 parametrÅ¯ Ã— 2 byty = 16 GB VRAM

Llama 4 8B @ Q4_K_M:
8,000,000,000 parametrÅ¯ Ã— 0.5 bytu = 4 GB VRAM
```

### Typy kvantizace

| Typ        | BitÅ¯ | Kvalita | PouÅ¾itÃ­            |
| ---------- | ---- | ------- | ------------------ |
| **FP16**   | 16   | 100%    | TrÃ©nink, reference |
| **Q8**     | 8    | ~99%    | TÃ©mÄ›Å™ bezztrÃ¡tovÃ©  |
| **Q4_K_M** | 4    | ~95-98% | â­ Sweet spot      |
| **Q2**     | 2    | ~70%    | Pouze experimenty  |

<Diagram type="vram-stack" />

---

## 4. VRAM kalkulaÄka ğŸ§®

### Vzorec

```text
CelkovÃ¡ VRAM â‰ˆ (Parametry Ã— Byty) + KV Cache + Aktivace

Pravidlo palce (Q4_K_M):
~0.7 GB na 1 miliardu parametrÅ¯ + 2-4 GB rezerva
```

### PÅ™ehled nÃ¡rokÅ¯ (Prosinec 2025)

| Kategorie     | Model       | VRAM (Q4) | TvÅ¯j HW                   |
| ------------- | ----------- | --------- | ------------------------- |
| **Tiny**      | Gemma 3 2B  | < 2 GB    | JakÃ½koli                  |
| **Standard**  | Llama 4 8B  | ~6 GB     | RTX 3060+                 |
| **Mid**       | Qwen 3 14B  | ~10 GB    | RTX 4070+                 |
| **Reasoning** | DeepSeek-R1 | ~12 GB    | RTX 4080+                 |
| **Large**     | Llama 4 70B | ~40 GB    | 2Ã— RTX 4090 nebo M4 Ultra |

<Callout type="warning">
  **Pozor na kontext!** Tabulka pÅ™edpoklÃ¡dÃ¡ 4-8k kontext. 128k kontext mÅ¯Å¾e
  spotÅ™ebovat desÃ­tky GB navÃ­c v KV Cache!
</Callout>

---

## 5. Latence: ProÄ lokÃ¡lnÄ› = okamÅ¾itÄ› âš¡

<Diagram type="latency-comparison" />

### Co tvoÅ™Ã­ latenci API?

1. **DNS lookup:** ~20ms
2. **TLS handshake:** ~50ms
3. **Network RTT:** ~50-100ms
4. **Queue time:** ~0-200ms (peak hours)
5. **Inference:** ~10-50ms

**Celkem:** 150-400ms

### LokÃ¡lnÃ­ latence

1. **Inference:** ~10-20ms

**Celkem:** 10-20ms

**25Ã— rychlejÅ¡Ã­!**

---

## 6. Ollama ekosystÃ©m ğŸ¦™

<Diagram type="ollama-ecosystem" />

### InferenÄnÃ­ enginy

| Engine        | NejlepÅ¡Ã­ pro          | PoznÃ¡mka                           |
| ------------- | --------------------- | ---------------------------------- |
| **Ollama**    | VÃ½vojÃ¡Å™i, zaÄÃ¡teÄnÃ­ci | "Docker pro AI"                    |
| **LM Studio** | VizuÃ¡lnÃ­ typy         | GUI, HuggingFace browser           |
| **vLLM**      | Produkce              | PagedAttention, vysokÃ¡ propustnost |
| **llama.cpp** | MaximÃ¡lnÃ­ vÃ½kon       | Low-level, C++                     |

### FormÃ¡ty souborÅ¯

<ConceptCard title="VÃ¡lka formÃ¡tÅ¯: GGUF vs Safetensors" icon="ğŸ“¦">

**GGUF:** KrÃ¡l lokÃ¡lnÃ­ inference. NavrÅ¾en pro llama.cpp. BalÃ­ vÃ¡hy + tokenizÃ©r do jednoho souboru. Podporuje memory mapping.

**Safetensors:** Standard HuggingFace. RychlÃ©, bezpeÄnÃ©, ale vyÅ¾aduje konverzi.

**Pravidlo:** Pro Ollama vÅ¾dy stahuj GGUF!

</ConceptCard>

---

## ğŸ§ª Lab 1: ZprovoznÄ›nÃ­ Ollama

### ğŸ¯ CÃ­l

Nainstalujte Ollama a spusÅ¥te prvnÃ­ lokÃ¡lnÃ­ inferenci.

### ğŸ“‹ Prerekvizity

- Linux, macOS nebo Windows (WSL2)
- 8+ GB RAM (16 GB doporuÄeno)
- GPU volitelnÃ© ale doporuÄenÃ©

<Callout type="warning">
  **PÅ™ed zaÄÃ¡tkem:** UjistÄ›te se, Å¾e mÃ¡te alespoÅˆ 10 GB volnÃ©ho mÃ­sta na disku
  pro modely.
</Callout>

### ğŸ› ï¸ Kroky

**Krok 1: Instalace**

```bash
# Linux / macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows: StÃ¡hnÄ›te z ollama.com
```

**Krok 2: OvÄ›Å™enÃ­**

```bash
ollama --version
# OÄekÃ¡vanÃ½ vÃ½stup: ollama version 0.5.x
```

**Krok 3: StaÅ¾enÃ­ modelu**

```bash
ollama pull llama4:scout
# Stahuje ~4.7 GB
```

**Krok 4: PrvnÃ­ chat**

```bash
ollama run llama4:scout
```

VyzkouÅ¡ejte:

- "NapiÅ¡ haiku o robotovi, kterÃ½ se uÄÃ­ milovat."
- "Kolik je 3 jablka minus 1 plus 2?"
- "NapiÅ¡ funkci v Pythonu pro Fibonacci sekvenci."

UkonÄenÃ­: `/bye`

**Krok 5: Monitoring**

```bash
# Linux (NVIDIA)
watch -n 1 nvidia-smi

# macOS
# Activity Monitor -> GPU History (Cmd+4)
```

### âœ… KritÃ©ria ÃºspÄ›chu

- [x] Ollama nainstalovÃ¡na
- [x] Model staÅ¾en a bÄ›Å¾Ã­
- [x] OdpovÄ›di se generujÃ­ lokÃ¡lnÄ› (zkuste odpojit wifi!)

<LabComplete labId="lab-ollama-setup" />

---

## ğŸ§ª Lab 2: Python integrace

### ğŸ¯ CÃ­l

OvlÃ¡dejte lokÃ¡lnÃ­ LLM programovÄ› pÅ™es Python.

### ğŸ“‹ Prerekvizity

- Python 3.10+
- Ollama bÄ›Å¾Ã­cÃ­ na pozadÃ­

### ğŸ› ï¸ Kroky

**Krok 1: ProstÅ™edÃ­**

```bash
mkdir local-agent && cd local-agent
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install ollama
```

**Krok 2: Skript**

VytvoÅ™te `agent.py`:

```python
import ollama
import time

MODEL = "llama4:scout"

def chat(prompt: str) -> str:
    print(f"\nğŸ¤– Sending to {MODEL}...\n")
    start = time.time()

    response = ""
    stream = ollama.chat(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    for chunk in stream:
        content = chunk["message"]["content"]
        print(content, end="", flush=True)
        response += content

    duration = time.time() - start
    tokens = len(response.split()) * 1.3  # ~estimate
    print(f"\n\nâš¡ {tokens/duration:.1f} tok/s | {duration:.1f}s")

    return response

if __name__ == "__main__":
    chat(input("Prompt: "))
```

**Krok 3: Test**

```bash
python agent.py
# Prompt: VysvÄ›tli rekurzi pÄ›tiletÃ©mu dÃ­tÄ›ti.
```

### ğŸ“ Troubleshooting

| ProblÃ©m            | Å˜eÅ¡enÃ­                                |
| ------------------ | ------------------------------------- |
| Connection refused | `ollama serve` v jinÃ©m terminÃ¡lu      |
| < 2 tok/s          | Model bÄ›Å¾Ã­ na CPU, zkuste menÅ¡Ã­ model |
| Out of memory      | Zkuste `gemma2:2b`                    |

### âœ… KritÃ©ria ÃºspÄ›chu

- [x] Skript bÄ›Å¾Ã­ bez chyb
- [x] VÃ½stup se streamuje v reÃ¡lnÃ©m Äase
- [x] VidÃ­te statistiky rychlosti

<LabComplete labId="lab-python-integration" />

---

## ğŸ§ª Lab 3: GenerovÃ¡nÃ­ obrÃ¡zkÅ¯ lokÃ¡lnÄ› ğŸ–¼ï¸

### ğŸ¯ CÃ­l

SpusÅ¥te lokÃ¡lnÃ­ image generation jako pÅ™Ã­pravu pro MCP integraci.

<Callout type="tip">
  **ProÄ to dÄ›lÃ¡me:** V Lekci 05 budete tento lokÃ¡lnÃ­ model pÅ™ipojovat pÅ™es MCP
  k vaÅ¡emu AI asistentovi!
</Callout>

### ğŸ“‹ Prerekvizity

- NVIDIA GPU s 8+ GB VRAM (doporuÄeno)
- Ollama nainstalovanÃ¡

### ğŸ› ï¸ Kroky

**Krok 1: StÃ¡hnÄ›te vision model**

```bash
ollama pull llava:13b
```

**Krok 2: Test multimodality**

```bash
# StÃ¡hnÄ›te testovacÃ­ obrÃ¡zek
curl -o test.jpg https://placekitten.com/400/300

# Analyzujte obrÃ¡zek
ollama run llava:13b "PopiÅ¡ tento obrÃ¡zek detailnÄ›" < test.jpg
```

**Krok 3: Pro plnou image generation (advanced)**

Pro Stable Diffusion / Flux.1 pouÅ¾ijte ComfyUI:

```bash
# Instalace ComfyUI (advanced)
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
pip install -r requirements.txt
python main.py
# OtevÅ™ete http://localhost:8188
```

### âœ… KritÃ©ria ÃºspÄ›chu

- [x] LLaVA model staÅ¾en
- [x] ÃšspÄ›Å¡nÃ¡ analÃ½za obrÃ¡zku
- [x] (Bonus) ComfyUI bÄ›Å¾Ã­

<LabComplete labId="lab-image-gen" />

---

## 7. Hybrid architektura: Kdy cloud, kdy local? ğŸ”„

Ne vÅ¡echno musÃ­ bÄ›Å¾et lokÃ¡lnÄ›. ChytrÃ¡ architektura kombinuje obojÃ­.

### Decision tree

| ScÃ©nÃ¡Å™                        | Å˜eÅ¡enÃ­    | DÅ¯vod              |
| ----------------------------- | --------- | ------------------ |
| CitlivÃ¡ data (PII, zdravotnÃ­) | **Local** | Compliance, GDPR   |
| VysokÃ½ objem (>1M tok/den)    | **Local** | NÃ¡klady            |
| Realtime (chatbot, IDE)       | **Local** | Latence            |
| One-off analÃ½za               | **Cloud** | PohodlÃ­            |
| Frontier capability (GPT-5)   | **Cloud** | NedostupnÃ© lokÃ¡lnÄ› |
| Prototyp / POC                | **Cloud** | Rychlost startu    |

<Callout type="tip">
  **Pro Tip:** ZaÄnÄ›te na cloudu pro POC, pÅ™esuÅˆte do lokÃ¡lu pro produkci.
  "Cloud for dev, local for prod."
</Callout>

---

## ğŸ† Holocron: KlÃ­ÄovÃ© poznatky

<ConceptCard title="Holocron: LokÃ¡lnÃ­ inteligence" icon="ğŸ’">

### ğŸ”‘ Key Takeaways

1. ğŸ§  **Modely jsou soubory.** GGUF balÃ­ vÃ¡hy + tokenizÃ©r. StaÄÃ­ stÃ¡hnout a spustit.
2. ğŸ’¾ **VRAM je limit.** Q4 kvantizace = 95% kvality za 60% pamÄ›ti. Sweet spot.
3. âš¡ **LokÃ¡lnÄ› = 25Ã— rychleji.** Å½Ã¡dnÃ¡ sÃ­Å¥, Å¾Ã¡dnÃ¡ queue. TTFT pod 20ms.
4. ğŸ’° **FixnÃ­ nÃ¡klady.** Po nÃ¡kupu HW platÃ­te jen elektÅ™inu. IdeÃ¡lnÃ­ pro agentic loops.

### ğŸ› ï¸ TvÅ¯j Local Stack 2025

```text
Hardware: RTX 4090 / Mac M4 Pro / Mac M4 Ultra
Engine:   Ollama (CLI) + Open WebUI (GUI)
Model:    Llama 4 8B (everyday) / 70B (complex)
Format:   GGUF Q4_K_M
```

### ğŸ“ RychlÃ½ start

```bash
# 1. Instalace
curl -fsSL https://ollama.com/install.sh | sh

# 2. Model
ollama pull llama4:scout

# 3. Chat
ollama run llama4:scout
```

</ConceptCard>

---

<Callout type="success">
  ğŸ‰ **Gratulace!** NynÃ­ mÃ¡te lokÃ¡lnÃ­ AI stack. V Lekci 05 ho propojÃ­me s MCP a
  udÄ›lÃ¡me z nÄ›j plnohodnotnÃ©ho agenta.
</Callout>

**DalÅ¡Ã­:** Lekce 05 â€” VÃ½voj s AI asistencÃ­ (MCP, Antigravity, Context Management)
