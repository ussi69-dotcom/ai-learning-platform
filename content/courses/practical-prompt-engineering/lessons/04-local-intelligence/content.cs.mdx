# LokÃ¡lnÃ­ inteligence ğŸ–¥ï¸

<Callout type="info">
**Mise:** OvlÃ¡dnÄ›te umÄ›nÃ­ provozovÃ¡nÃ­ nejmodernÄ›jÅ¡Ã­ch LLM na vlastnÃ­m hardwaru. ZÃ­skÃ¡te maximÃ¡lnÃ­ soukromÃ­, nulovou latenci a fixnÃ­ nÃ¡klady.

â³ **ÄŒas ÄtenÃ­:** 40 min | ğŸ§ª **[3] Laby souÄÃ¡stÃ­**
</Callout>

<VideoSwitcher alternatives={[{"id":"Wjrdr0NU4Sk","title":"Host ALL your AI locally","author":"NetworkChuck","lang":"en"},{"id":"sfilT8m73xM","title":"Use Llama 3 Locally with Ollama","author":"David Mbugua","lang":"en"},{"id":"W8CObaM-gjA","title":"DeepSeek R1 Full Local Review","author":"Matthew Berman","lang":"en"},{"id":"zRECqmt9LlE","title":"ChatGPT na vaÅ¡em poÄÃ­taÄi? Ollama","author":"AI s rozumem","lang":"cs"}]} />

## âš¡ Deklarace AI nezÃ¡vislosti

**Co kdyby kaÅ¾dÃ½ AI rozhovor, kterÃ½ jste kdy vedli, zÅ¯stal navÅ¾dy na vaÅ¡em poÄÃ­taÄi?**

VÄ›tÅ¡ina promptÅ¯ do cloudovÃ½ch modelÅ¯ opouÅ¡tÃ­ vaÅ¡e zaÅ™Ã­zenÃ­. **SpotÅ™ebitelskÃ© nÃ¡stroje mohou data pouÅ¾Ã­vat k vylepÅ¡ovÃ¡nÃ­ modelÅ¯**, zatÃ­mco enterprise nabÃ­dky Äasto optâ€‘outujÃ­ â€” vÅ¾dy ovÄ›Å™te firemnÃ­ politiku.

Ale nemusÃ­ to tak bÃ½t.

V letech 2024â€“2025 lokÃ¡lnÃ­ AI modely pÅ™ekroÄily kritickÃ½ prÃ¡h: **lokÃ¡lnÃ­ provoz je koneÄnÄ› praktickÃ½ pro reÃ¡lnou prÃ¡ci**. Rodiny jako **Meta Llama 3**, **Alibaba Qwen2** a **DeepSeekâ€‘R1** pÅ™inesly solidnÃ­ reasoning i na osobnÃ­ hardware.

<Callout type="warning">
**Realita nÃ¡kladÅ¯:** Cloud pricing se Äasto mÄ›nÃ­. LokÃ¡lnÃ­ provoz mÃ¡ vyÅ¡Å¡Ã­ vstupnÃ­ nÃ¡klad, ale pÅ™edvÃ­datelnÃ½ marginÃ¡l. PÅ™i vysokÃ©m objemu se breakâ€‘even dostavÃ­ rychle.
</Callout>

---

## 1. ProÄ spouÅ¡tÄ›t modely lokÃ¡lnÄ›? ğŸ°

V Ã©Å™e dominance API jako OpenAI a Anthropic hnutÃ­ k **LokÃ¡lnÃ­ Inteligenci** explodovalo. ProÄ se obtÄ›Å¾ovat vlastnÃ­ infrastrukturou?

<Diagram type="local-llm-architecture" />

### 4 kritickÃ© dÅ¯vody

| DÅ¯vod           | Cloud API              | LokÃ¡lnÃ­ LLM                |
| --------------- | ---------------------- | -------------------------- |
| **ğŸ”’ SoukromÃ­** | Data opouÅ¡tÃ­ perimetr  | Data nikdy neopustÃ­ RAM    |
| **ğŸ’° NÃ¡klady**  | VariabilnÃ­, mÄ›nÃ­ se Äasto | FixnÃ­ (HW + elektÅ™ina)     |
| **âš¡ Latence**  | 150-400ms              | 10-20ms                    |
| **ğŸ›ï¸ Kontrola** | Censored, RLHF         | PlnÃ¡ kontrola, fine-tuning |

---

## 2. Rodiny modelÅ¯, kterÃ© stojÃ­ za pozornost (2024â€“2025) ğŸ“Š

Openâ€‘weight modely jdou rychle dopÅ™edu. DominujÃ­ tÅ™i rodiny:

- **Meta Llama 3:** SilnÃ© univerzÃ¡ly s obrovskou komunitou.
- **Alibaba Qwen2:** VÃ½bornÃ© instrukÄnÃ­ modely, silnÃ¡ vÃ­cejazyÄnost.
- **DeepSeekâ€‘R1:** ZamÄ›Å™eno na reasoning.

<Diagram type="model-benchmark-chart" />

### PraktickÃ½ vÃ½bÄ›r (rule of thumb)

| TÅ™Ã­da | PÅ™Ã­klad | TypickÃ¡ VRAM (4â€‘bit) | VhodnÃ© pro |
|------|---------|-----------------------|------------|
| **Tiny** | 2â€“4B modely | 2â€“4 GB | RychlÃ© Ãºkoly, lehkÃ© agenty |
| **Standard** | Llama 3 8B / Qwen2 7B | 6â€“8 GB | DennÃ­ prÃ¡ce |
| **Mid** | Qwen2 14B | 10â€“12 GB | LepÅ¡Ã­ kvalita psanÃ­/kÃ³du |
| **Reasoning** | DeepSeekâ€‘R1 | 12+ GB | VÃ­cekrokovÃ© uvaÅ¾ovÃ¡nÃ­ |
| **Large** | 70B tÅ™Ã­da | 40+ GB | ServerovÃ© nasazenÃ­ |

<Callout type="tip">
**Sweet spot:** 7â€“8B modely na bÄ›Å¾nÃ½ch GPU; 13â€“14B pokud mÃ¡Å¡ 12 GB+ VRAM.
</Callout>

<Callout type="info">
**Zdroje:** Meta Llama 3 (2024â€‘04â€‘18), Alibaba Qwen2 (2024â€‘06â€‘11), DeepSeekâ€‘R1 (2024â€‘07â€‘25).
</Callout>

---

## 3. Kvantizace: Magie komprese ğŸ“‰

Pro spuÅ¡tÄ›nÃ­ modelu musÃ­te naÄÃ­st jeho **vÃ¡hy** do VRAM. StandardnÃ­ trÃ©nink probÃ­hÃ¡ v **FP16** (16-bit).

### Matematika

```text
Llama 3 8B @ FP16:
8,000,000,000 parametrÅ¯ Ã— 2 byty = 16 GB VRAM

Llama 3 8B @ Q4_K_M:
8,000,000,000 parametrÅ¯ Ã— 0.5 bytu = 4 GB VRAM
```

### Typy kvantizace

| Typ        | BitÅ¯ | Kvalita | PouÅ¾itÃ­            |
| ---------- | ---- | ------- | ------------------ |
| **FP16**   | 16   | 100%    | TrÃ©nink, reference |
| **Q8**     | 8    | ~99%    | TÃ©mÄ›Å™ bezztrÃ¡tovÃ©  |
| **Q4_K_M** | 4    | ~95-98% | â­ Sweet spot      |
| **Q2**     | 2    | ~70%    | Pouze experimenty  |

<Diagram type="vram-stack" />

---

## 4. VRAM kalkulaÄka ğŸ§®

### Vzorec

```text
CelkovÃ¡ VRAM â‰ˆ (Parametry Ã— Byty) + KV Cache + Aktivace

Pravidlo palce (Q4_K_M):
~0.7 GB na 1 miliardu parametrÅ¯ + 2-4 GB rezerva
```

### PÅ™ehled nÃ¡rokÅ¯ (Prosinec 2025)

| Kategorie     | Model       | VRAM (Q4) | TvÅ¯j HW                   |
| ------------- | ----------- | --------- | ------------------------- |
| **Tiny**      | Gemma 3 2B  | < 2 GB    | JakÃ½koli                  |
| **Standard**  | Llama 3 8B  | ~6 GB     | RTX 3060+                 |
| **Mid**       | Qwen2 14B   | ~10 GB    | RTX 4070+                 |
| **Reasoning** | DeepSeek-R1 | ~12 GB    | RTX 4080+                 |
| **Large**     | Llama 3 70B | ~40 GB    | 2Ã— RTX 4090 nebo M4 Ultra |

<Callout type="warning">
  **Pozor na kontext!** Tabulka pÅ™edpoklÃ¡dÃ¡ 4-8k kontext. 128k kontext mÅ¯Å¾e
  spotÅ™ebovat desÃ­tky GB navÃ­c v KV Cache!
</Callout>

---

## 5. Latence: ProÄ lokÃ¡lnÄ› = okamÅ¾itÄ› âš¡

<Diagram type="latency-comparison" />

### Co tvoÅ™Ã­ latenci API?

1. **DNS lookup:** ~20ms
2. **TLS handshake:** ~50ms
3. **Network RTT:** ~50-100ms
4. **Queue time:** ~0-200ms (peak hours)
5. **Inference:** ~10-50ms

**Celkem:** 150-400ms

### LokÃ¡lnÃ­ latence

1. **Inference:** ~10-20ms

**Celkem:** 10-20ms

**25Ã— rychlejÅ¡Ã­!**

---

## 6. Ollama ekosystÃ©m ğŸ¦™

<Diagram type="ollama-ecosystem" />

### InferenÄnÃ­ enginy

| Engine        | NejlepÅ¡Ã­ pro          | PoznÃ¡mka                           |
| ------------- | --------------------- | ---------------------------------- |
| **Ollama**    | VÃ½vojÃ¡Å™i, zaÄÃ¡teÄnÃ­ci | "Docker pro AI"                    |
| **LM Studio** | VizuÃ¡lnÃ­ typy         | GUI, HuggingFace browser           |
| **vLLM**      | Produkce              | PagedAttention, vysokÃ¡ propustnost |
| **llama.cpp** | MaximÃ¡lnÃ­ vÃ½kon       | Low-level, C++                     |

### FormÃ¡ty souborÅ¯

<ConceptCard title="VÃ¡lka formÃ¡tÅ¯: GGUF vs Safetensors" icon="ğŸ“¦">

**GGUF:** KrÃ¡l lokÃ¡lnÃ­ inference. NavrÅ¾en pro llama.cpp. BalÃ­ vÃ¡hy + tokenizÃ©r do jednoho souboru. Podporuje memory mapping.

**Safetensors:** Standard HuggingFace. RychlÃ©, bezpeÄnÃ©, ale vyÅ¾aduje konverzi.

**Pravidlo:** Pro Ollama vÅ¾dy stahuj GGUF!

</ConceptCard>

---

## ğŸ§ª Lab 1: ZprovoznÄ›nÃ­ Ollama

### ğŸ¯ CÃ­l

Nainstalujte Ollama a spusÅ¥te prvnÃ­ lokÃ¡lnÃ­ inferenci.

### ğŸ“‹ Prerekvizity

- Linux, macOS nebo Windows (WSL2)
- 8+ GB RAM (16 GB doporuÄeno)
- GPU volitelnÃ© ale doporuÄenÃ©

<Callout type="warning">
  **PÅ™ed zaÄÃ¡tkem:** UjistÄ›te se, Å¾e mÃ¡te alespoÅˆ 10 GB volnÃ©ho mÃ­sta na disku
  pro modely.
</Callout>

### ğŸ› ï¸ Kroky

**Krok 1: Instalace**

```bash
# Linux / macOS
curl -fsSL https://ollama.com/install.sh | sh

# Windows: StÃ¡hnÄ›te z ollama.com
```

**Krok 2: OvÄ›Å™enÃ­**

```bash
ollama --version
# OÄekÃ¡vanÃ½ vÃ½stup: ollama version 0.5.x
```

**Krok 3: StaÅ¾enÃ­ modelu**

```bash
ollama pull llama4:scout
# Stahuje ~4.7 GB
```

**Krok 4: PrvnÃ­ chat**

```bash
ollama run llama4:scout
```

VyzkouÅ¡ejte:

- "NapiÅ¡ haiku o robotovi, kterÃ½ se uÄÃ­ milovat."
- "Kolik je 3 jablka minus 1 plus 2?"
- "NapiÅ¡ funkci v Pythonu pro Fibonacci sekvenci."

UkonÄenÃ­: `/bye`

**Krok 5: Monitoring**

```bash
# Linux (NVIDIA)
watch -n 1 nvidia-smi

# macOS
# Activity Monitor -> GPU History (Cmd+4)
```

### âœ… KritÃ©ria ÃºspÄ›chu

- [x] Ollama nainstalovÃ¡na
- [x] Model staÅ¾en a bÄ›Å¾Ã­
- [x] OdpovÄ›di se generujÃ­ lokÃ¡lnÄ› (zkuste odpojit wifi!)

<LabComplete labId="lab-ollama-setup" />

---

## ğŸ§ª Lab 2: Python integrace

### ğŸ¯ CÃ­l

OvlÃ¡dejte lokÃ¡lnÃ­ LLM programovÄ› pÅ™es Python.

### ğŸ“‹ Prerekvizity

- Python 3.10+
- Ollama bÄ›Å¾Ã­cÃ­ na pozadÃ­

### ğŸ› ï¸ Kroky

**Krok 1: ProstÅ™edÃ­**

```bash
mkdir local-agent && cd local-agent
python3 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install ollama
```

**Krok 2: Skript**

VytvoÅ™te `agent.py`:

```python
import ollama
import time

MODEL = "llama4:scout"

def chat(prompt: str) -> str:
    print(f"\nğŸ¤– Sending to {MODEL}...\n")
    start = time.time()

    response = ""
    stream = ollama.chat(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    for chunk in stream:
        content = chunk["message"]["content"]
        print(content, end="", flush=True)
        response += content

    duration = time.time() - start
    tokens = len(response.split()) * 1.3  # ~estimate
    print(f"\n\nâš¡ {tokens/duration:.1f} tok/s | {duration:.1f}s")

    return response

if __name__ == "__main__":
    chat(input("Prompt: "))
```

**Krok 3: Test**

```bash
python agent.py
# Prompt: VysvÄ›tli rekurzi pÄ›tiletÃ©mu dÃ­tÄ›ti.
```

### ğŸ“ Troubleshooting

| ProblÃ©m            | Å˜eÅ¡enÃ­                                |
| ------------------ | ------------------------------------- |
| Connection refused | `ollama serve` v jinÃ©m terminÃ¡lu      |
| < 2 tok/s          | Model bÄ›Å¾Ã­ na CPU, zkuste menÅ¡Ã­ model |
| Out of memory      | Zkuste `gemma2:2b`                    |

### âœ… KritÃ©ria ÃºspÄ›chu

- [x] Skript bÄ›Å¾Ã­ bez chyb
- [x] VÃ½stup se streamuje v reÃ¡lnÃ©m Äase
- [x] VidÃ­te statistiky rychlosti

<LabComplete labId="lab-python-integration" />

---

## ğŸ§ª Lab 3: GenerovÃ¡nÃ­ obrÃ¡zkÅ¯ lokÃ¡lnÄ› ğŸ–¼ï¸

### ğŸ¯ CÃ­l

SpusÅ¥te lokÃ¡lnÃ­ image generation jako pÅ™Ã­pravu pro MCP integraci.

<Callout type="tip">
  **ProÄ to dÄ›lÃ¡me:** V Lekci 05 budete tento lokÃ¡lnÃ­ model pÅ™ipojovat pÅ™es MCP
  k vaÅ¡emu AI asistentovi!
</Callout>

### ğŸ“‹ Prerekvizity

- NVIDIA GPU s 8+ GB VRAM (doporuÄeno)
- Ollama nainstalovanÃ¡

### ğŸ› ï¸ Kroky

**Krok 1: StÃ¡hnÄ›te vision model**

```bash
ollama pull llava:13b
```

**Krok 2: Test multimodality**

```bash
# StÃ¡hnÄ›te testovacÃ­ obrÃ¡zek
curl -o test.jpg https://placekitten.com/400/300

# Analyzujte obrÃ¡zek
ollama run llava:13b "PopiÅ¡ tento obrÃ¡zek detailnÄ›" < test.jpg
```

**Krok 3: Pro plnou image generation (advanced)**

Pro Stable Diffusion / Flux.1 pouÅ¾ijte ComfyUI:

```bash
# Instalace ComfyUI (advanced)
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
pip install -r requirements.txt
python main.py
# OtevÅ™ete http://localhost:8188
```

### âœ… KritÃ©ria ÃºspÄ›chu

- [x] LLaVA model staÅ¾en
- [x] ÃšspÄ›Å¡nÃ¡ analÃ½za obrÃ¡zku
- [x] (Bonus) ComfyUI bÄ›Å¾Ã­

<LabComplete labId="lab-image-gen" />

---

## 7. Hybrid architektura: Kdy cloud, kdy local? ğŸ”„

Ne vÅ¡echno musÃ­ bÄ›Å¾et lokÃ¡lnÄ›. ChytrÃ¡ architektura kombinuje obojÃ­.

### Decision tree

| ScÃ©nÃ¡Å™                        | Å˜eÅ¡enÃ­    | DÅ¯vod              |
| ----------------------------- | --------- | ------------------ |
| CitlivÃ¡ data (PII, zdravotnÃ­) | **Local** | Compliance, GDPR   |
| VysokÃ½ objem (>1M tok/den)    | **Local** | NÃ¡klady            |
| Realtime (chatbot, IDE)       | **Local** | Latence            |
| One-off analÃ½za               | **Cloud** | PohodlÃ­            |
| Frontier capability (GPT-5)   | **Cloud** | NedostupnÃ© lokÃ¡lnÄ› |
| Prototyp / POC                | **Cloud** | Rychlost startu    |

<Callout type="tip">
  **Pro Tip:** ZaÄnÄ›te na cloudu pro POC, pÅ™esuÅˆte do lokÃ¡lu pro produkci.
  "Cloud for dev, local for prod."
</Callout>

---

## ğŸ† Holocron: KlÃ­ÄovÃ© poznatky

<ConceptCard title="Holocron: LokÃ¡lnÃ­ inteligence" icon="ğŸ’">

### ğŸ”‘ Key Takeaways

1. ğŸ§  **Modely jsou soubory.** GGUF balÃ­ vÃ¡hy + tokenizÃ©r. StaÄÃ­ stÃ¡hnout a spustit.
2. ğŸ’¾ **VRAM je limit.** Q4 kvantizace = 95% kvality za 60% pamÄ›ti. Sweet spot.
3. âš¡ **LokÃ¡lnÄ› = 25Ã— rychleji.** Å½Ã¡dnÃ¡ sÃ­Å¥, Å¾Ã¡dnÃ¡ queue. TTFT pod 20ms.
4. ğŸ’° **FixnÃ­ nÃ¡klady.** Po nÃ¡kupu HW platÃ­te jen elektÅ™inu. IdeÃ¡lnÃ­ pro agentic loops.

### ğŸ› ï¸ TvÅ¯j Local Stack 2025

```text
Hardware: RTX 4090 / Mac M4 Pro / Mac M4 Ultra
Engine:   Ollama (CLI) + Open WebUI (GUI)
Model:    Llama 3 8B (everyday) / 70B (complex)
Format:   GGUF Q4_K_M
```

### ğŸ“ RychlÃ½ start

```bash
# 1. Instalace
curl -fsSL https://ollama.com/install.sh | sh

# 2. Model
ollama pull llama4:scout

# 3. Chat
ollama run llama4:scout
```

</ConceptCard>

---

<Callout type="success">
  ğŸ‰ **Gratulace!** NynÃ­ mÃ¡te lokÃ¡lnÃ­ AI stack. V Lekci 05 ho propojÃ­me s MCP a
  udÄ›lÃ¡me z nÄ›j plnohodnotnÃ©ho agenta.
</Callout>

**DalÅ¡Ã­:** Lekce 05 â€” VÃ½voj s AI asistencÃ­ (MCP, Antigravity, Context Management)
