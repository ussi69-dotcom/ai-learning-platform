# Fine-Tuning: NauÄte AI mluvit VAÅ ÃM jazykem

> **OpenAI ÃºÄtuje zhruba $5 za fine-tuning GPT-4.1 mini na 1 milion trÃ©novacÃ­ch tokenÅ¯. Ale tady je tajemstvÃ­: vÄ›tÅ¡ina projektÅ¯ to nepotÅ™ebuje.**

<Callout type="info">
**VaÅ¡e mise:** OvlÃ¡dnÄ›te umÄ›nÃ­ fine-tuningu. NauÄte se kdy ho pouÅ¾Ã­t, kdy se mu vyhnout, a jak pÅ™ipravit dataset pro nauÄenÃ­ AI modelu novÃ½m dovednostem bez rozbitÃ­ rozpoÄtu.

â³ **ÄŒas ÄtenÃ­:** 35 min | ğŸ§ª **[2] Laby souÄÃ¡stÃ­**
</Callout>

<VideoSwitcher alternatives={[
  {"id":"_2p0BR4xc5E","title":"Fine-Tuning GPT Models (OpenAI)"},
  {"id":"gyGNQrwU1Kg","title":"When NOT to Fine-Tune (AI Explained)"}
]} />

---

## VelkÃ¡ debata: Fine-Tuning vs. svÄ›t

NepouÅ¾ili byste kladivo na rozbÃ­jenÃ­ oÅ™echÅ¯, Å¾e? StejnÃ© platÃ­ pro AI. Fine-tuning je silnÃ½, ale nenÃ­ to vaÅ¡e jedinÃ¡ moÅ¾nost. PojÄme si rozebrat hlavnÃ­ hrÃ¡Äe ve hÅ™e pÅ™izpÅ¯sobenÃ­ modelÅ¯.

| Vlastnost | Prompt Engineering | RAG | Fine-Tuning |
|-----------|-------------------|-----|-------------|
| **Co to je** | Tvorba detailnÃ­ch instrukcÃ­ | PoskytnutÃ­ AI pÅ™Ã­stupu k externÃ­m znalostem | Aktualizace vnitÅ™nÃ­ch vah modelu |
| **NejlepÅ¡Ã­ pro** | JednoduchÃ© Ãºkoly, stylovÃ© pokyny | ZnalostnÄ› nÃ¡roÄnÃ© Ãºkoly | UÄenÃ­ novÃ½ch dovednostÃ­ nebo stylÅ¯ |
| **NÃ¡klady** | NÃ­zkÃ© (jen tokeny) | StÅ™ednÃ­ (tokeny + vektorovÃ¡ DB) | VysokÃ© (trÃ©nink + prÃ©miovÃ© pouÅ¾itÃ­) |
| **SloÅ¾itost** | NÃ­zkÃ¡ | StÅ™ednÃ­ | VysokÃ¡ |
| **Analogie** | PovÃ­dÃ¡nÃ­ pÅ™es terminÃ¡l | USB disk s daty | PÃ¡jenÃ­ novÃ©ho Äipu |

<Callout type="tip">
**Pro Tip:** VÅ¾dy zaÄnÄ›te s Prompt Engineeringem. KdyÅ¾ to selÅ¾e, zkuste RAG. Pouze kdyÅ¾ obojÃ­ nestaÄÃ­, zvaÅ¾te tÄ›Å¾kÃ© dÄ›lostÅ™electvo: Fine-Tuning.
</Callout>

---

## Kdy je Fine-Tuning vÃ¡Å¡ Jedi Mistr

Fine-tuning nenÃ­ o vklÃ¡dÃ¡nÃ­ novÃ½ch faktÅ¯. Jde o uÄenÃ­ **dovednosti**. PÅ™edstavte si to jako poslÃ¡nÃ­ Padawana do JediskÃ©ho chrÃ¡mu. NeuÄÃ­ se novÃ© informace o galaxii; uÄÃ­ se jak *pouÅ¾Ã­vat* SÃ­lu.

<ConceptCard title="5 perfektnÃ­ch pÅ™Ã­padÅ¯ pro Fine-Tuning" icon="ğŸ¯">

**1. ZvlÃ¡dnutÃ­ specifickÃ©ho stylu nebo hlasu** ğŸ¨
VaÅ¡e firma mÃ¡ jedineÄnÃ½ komunikaÄnÃ­ styl. Fine-tuning perfektnÄ› trefÃ­ hlas znaÄky.

**2. DodrÅ¾enÃ­ specifickÃ©ho formÃ¡tu** ğŸ“
KomplexnÃ­ YAML, vlastnÃ­ Ãºryvky kÃ³du, strukturovanÃ© lÃ©kaÅ™skÃ© zprÃ¡vy - kdyÅ¾ zÃ¡leÅ¾Ã­ na formÃ¡tu.

**3. ZlepÅ¡enÃ­ spolehlivosti na ÃºzkÃ½ch Ãºkolech** ğŸ¯
DomÃ©novÃ½ expert pro finanÄnÃ­ sentiment, prÃ¡vnÃ­ analÃ½zu, technickou podporu.

**4. ZvlÃ¡dÃ¡nÃ­ hraniÄnÃ­ch pÅ™Ã­padÅ¯** âš™ï¸
ZÃ¡ludnÃ© situace, kde vÃ¡Å¡ zÃ¡kladnÃ­ model selhÃ¡vÃ¡? VytvoÅ™te dataset a trÃ©nujte!

**5. Å˜etÄ›zenÃ­ komplexnÃ­ch krokÅ¯** ğŸ”—
Sumarizuj â†’ Extrahuj entity â†’ FormÃ¡tuj jako JSON - vÅ¡e jako jedna dovednost.

</ConceptCard>

<Callout type="warning">
**VarovÃ¡nÃ­:** Nikdy nepouÅ¾Ã­vejte fine-tuning pro pÅ™idÃ¡nÃ­ novÃ½ch znalostÃ­. Pokud vÃ¡Å¡ model nevÃ­ o novÃ©m produktovÃ©m launchi, pouÅ¾ijte RAG. Fine-tuning pro znalosti je drahÃ½, neefektivnÃ­ a nÃ¡chylnÃ½ k selhÃ¡nÃ­.
</Callout>

---

## JSONL evangelium: PÅ™Ã­prava vaÅ¡ich posvÃ¡tnÃ½ch textÅ¯

VÃ¡Å¡ fine-tuning je pouze tak dobrÃ½ jako vaÅ¡e data. **Odpad dovnitÅ™, odpad ven.** StandardnÃ­ formÃ¡t je **JSONL (JSON Lines)** - textovÃ½ soubor, kde kaÅ¾dÃ½ Å™Ã¡dek je platnÃ½ JSON objekt.

```json
{"messages": [{"role": "system", "content": "Jsi pomocnÃ½ asistent, kterÃ½ generuje pirÃ¡tskÃ½ slang."}, {"role": "user", "content": "PÅ™eloÅ¾: Ahoj, pÅ™Ã­teli. Jak se mÃ¡Å¡?"}, {"role": "assistant", "content": "Ahoj, nÃ¡moÅ™nÃ­ku! Jak se ti daÅ™Ã­?"}]}
{"messages": [{"role": "system", "content": "Jsi pomocnÃ½ asistent, kterÃ½ generuje pirÃ¡tskÃ½ slang."}, {"role": "user", "content": "PÅ™eloÅ¾: Moje auto je rychlÃ©."}, {"role": "assistant", "content": "MÃ¡ loÄ je bystrÃ¡ plavidlo!"}]}
```

### PoÅ¾adavky na dataset

| Aspekt | Minimum | DoporuÄeno | PoznÃ¡mky |
|--------|---------|-------------|----------|
| **PÅ™Ã­kladÅ¯** | 10 | 50-100 | VÃ­ce je lepÅ¡Ã­ pro komplexnÃ­ Ãºkoly |
| **Kvalita** | ÄŒistÃ¡ | OvÄ›Å™eno ÄlovÄ›kem | IdeÃ¡lnÄ› vÃ­ce recenzentÅ¯ |
| **Rozmanitost** | NÄ›jakÃ¡ variace | Å irokÃ© pokrytÃ­ | Vyhnout se overfittingu |
| **FormÃ¡t** | KonzistentnÃ­ | IdentickÃ¡ struktura | StejnÃ½ system prompt |

---

## ğŸ”¬ Lab 1: VytvoÅ™te svÅ¯j trÃ©novacÃ­ dataset

VÃ¡Å¡ prvnÃ­ Ãºkol, mladÃ½ Padawane. VytvoÅ™te soubor `dataset.jsonl` pro nauÄenÃ­ AI bÃ½t "sumarizÃ¡torem kÃ³du", kterÃ½ vysvÄ›tluje Python kÃ³d jednoduchÃ½mi slovy.

**CÃ­l:** VytvoÅ™it platnÃ½ JSONL soubor s 10 trÃ©novacÃ­mi pÅ™Ã­klady.

<Steps>

### Krok 1: VytvoÅ™te soubor
VytvoÅ™te novÃ½ soubor `dataset.jsonl` ve vaÅ¡em oblÃ­benÃ©m textovÃ©m editoru.

### Krok 2: Definujte System Message
VaÅ¡e systÃ©movÃ¡ zprÃ¡va by mÄ›la bÃ½t stejnÃ¡ pro vÅ¡echny pÅ™Ã­klady:
```text
Jsi expert Python programÃ¡tor, kterÃ½ vysvÄ›tluje kÃ³d jednoduchÃ½mi, snadno pochopitelnÃ½mi slovy.
```

### Krok 3: PÅ™idejte 10 trÃ©novacÃ­ch pÅ™Ã­kladÅ¯
KaÅ¾dÃ½ Å™Ã¡dek je jeden pÅ™Ã­klad. UÅ¾ivatel poskytne kÃ³d, asistent ho vysvÄ›tlÃ­.

**PÅ™Ã­klad:**
```json
{"messages": [{"role": "system", "content": "Jsi expert Python programÃ¡tor, kterÃ½ vysvÄ›tluje kÃ³d jednoduchÃ½mi, snadno pochopitelnÃ½mi slovy."}, {"role": "user", "content": "def add(a, b):\n  return a + b"}, {"role": "assistant", "content": "Toto je jednoduchÃ¡ funkce, kterÃ¡ vezme dvÄ› ÄÃ­sla, `a` a `b`, a vrÃ¡tÃ­ jejich souÄet."}]}
```

### Krok 4: Validujte vÃ¡Å¡ JSONL
PouÅ¾ijte online JSON validÃ¡tor nebo spusÅ¥te:
```bash
python -c "import json; [json.loads(line) for line in open('dataset.jsonl')]"
```

</Steps>

**ğŸ’¡ Aha Moment:** "Jedna Å¡patnÄ› umÃ­stÄ›nÃ¡ ÄÃ¡rka mÅ¯Å¾e zniÄit celÃ½ dataset. Validace JSONL je povinnÃ¡!"

<LabComplete labId="fine-tuning-dataset" />

---

## OpenAI Fine-Tuning API: PosvÃ¡tnÃ½ rituÃ¡l

Jakmile jsou vaÅ¡e data pÅ™ipravena, mÅ¯Å¾ete zaÄÃ­t fine-tuning rituÃ¡l. Je to vÃ­cekrokovÃ½ proces, ale pÅ™ekvapivÄ› pÅ™Ã­moÄarÃ½.

<Steps>

### Krok 1: Nainstalujte OpenAI knihovnu
```bash
pip install --upgrade openai
```

### Krok 2: Nahrajte vÃ¡Å¡ dataset
```python
from openai import OpenAI
client = OpenAI()  # OPENAI_API_KEY musÃ­ bÃ½t nastavena

file_upload = client.files.create(
    file=open("dataset.jsonl", "rb"),
    purpose="fine-tune"
)
print(f"Soubor nahrÃ¡n s ID: {file_upload.id}")
```

### Krok 3: VytvoÅ™te Fine-Tuning job
```python
job = client.fine_tuning.jobs.create(
    training_file=file_upload.id,
    model="gpt-4.1-mini",  # Nebo "gpt-4.1" pro vyÅ¡Å¡Ã­ kvalitu
    hyperparameters={"n_epochs": 3}
)
print(f"Job vytvoÅ™en s ID: {job.id}")
```

### Krok 4: Sledujte job
```python
status = client.fine_tuning.jobs.retrieve(job.id)
print(f"Status: {status.status}")

# Seznam udÃ¡lostÃ­
events = client.fine_tuning.jobs.list_events(
    fine_tuning_job_id=job.id,
    limit=10
)
for event in events:
    print(event.message)
```

### Krok 5: PouÅ¾ijte vÃ¡Å¡ novÃ½ model!
```python
completion = client.chat.completions.create(
    model="ft:gpt-4.1-mini:your-org:custom:abc123",  # NÃ¡zev vaÅ¡eho modelu
    messages=[
        {"role": "system", "content": "Jsi expert Python programÃ¡tor..."},
        {"role": "user", "content": "def hello():\n  print('Hello, World!')"}
    ]
)
print(completion.choices[0].message.content)
```

</Steps>

---

## Ekonomika: StojÃ­ to za kredity?

Fine-tuning nenÃ­ zadarmo. PlatÃ­te za trÃ©nink **A** prÃ©miovÃ© sazby za pouÅ¾itÃ­. PojÄme si to rozebrat:

### CenÃ­k GPT-4.1 mini (k roku 2025)

| Typ nÃ¡kladu | Cena za 1M tokenÅ¯ |
|-------------|-------------------|
| **TrÃ©nink** | $5.00 |
| **Input (fine-tuned)** | $0.80 |
| **Output (fine-tuned)** | $3.20 |

Srovnejte s cenami zÃ¡kladnÃ­ho modelu na OpenAI pricing page. **Fine-tuned modely obvykle nesou prÃ©mii.**

---

## ğŸ”¬ Lab 2: VÃ½zva kalkulaÄky nÃ¡kladÅ¯

Firma chce vybudovat chatbota pro 5 000 zÃ¡kaznickÃ½ch dotazÅ¯ dennÄ›. RAG nestaÄÃ­ - potÅ™ebujÃ­ fine-tuning.

**ZadÃ¡nÃ­:**
- PrÅ¯mÄ›rnÃ½ dotaz: 150 tokenÅ¯ input, 300 tokenÅ¯ output
- TrÃ©novacÃ­ data: 2 000 pÅ™Ã­kladÅ¯ Ã— 500 tokenÅ¯ kaÅ¾dÃ½
- Epochy: 3

**VÃ¡Å¡ Ãºkol:** VypoÄÃ­tejte celkovÃ© nÃ¡klady prvnÃ­ho mÄ›sÃ­ce.

<Steps>

### Krok 1: VypoÄÃ­tejte trÃ©novacÃ­ tokeny
```
Tokeny datasetu = 2 000 Ã— 500 = 1 000 000 tokenÅ¯
TrÃ©novacÃ­ tokeny = 1 000 000 Ã— 3 epochy = 3 000 000 tokenÅ¯
```

### Krok 2: VypoÄÃ­tejte nÃ¡klady na trÃ©nink
```
NÃ¡klady na trÃ©nink = (3 000 000 / 1 000 000) Ã— $5 = $15.00
```

### Krok 3: VypoÄÃ­tejte dennÃ­ pouÅ¾itÃ­
```
DennÃ­ Input = 5 000 Ã— 150 = 750 000 tokenÅ¯
DennÃ­ Output = 5 000 Ã— 300 = 1 500 000 tokenÅ¯

DennÃ­ Input nÃ¡klady = (0.75M / 1M) Ã— $0.80 = $0.60
DennÃ­ Output nÃ¡klady = (1.5M / 1M) Ã— $3.20 = $4.80
DennÃ­ celkem = $5.40
```

### Krok 4: VypoÄÃ­tejte mÄ›sÃ­ÄnÃ­ celkem
```
MÄ›sÃ­ÄnÃ­ pouÅ¾itÃ­ = $5.40 Ã— 30 = $162.00
Celkem prvnÃ­ mÄ›sÃ­c = $15 + $162.00 = $177.00
```

</Steps>

**ğŸ’¡ Aha Moment:** "$135.75/mÄ›sÃ­c za specializovanÃ©ho, spolehlivÃ©ho chatbota, kterÃ½ potÄ›Å¡Ã­ zÃ¡kaznÃ­ky? Pro mnoho firem je to jasnÃ¡ nÃ¡vratnost investice!"

<LabComplete labId="fine-tuning-cost" />

---

## ÄŒastÃ© pasti: TemnÃ¡ strana SÃ­ly

<Callout type="warning">

### ğŸš¨ 4 smrtelnÃ© hÅ™Ã­chy Fine-Tuningu

1. **Overfitting** - Model se stane jednoÃºÄelovÃ½m. DrÅ¾te `n_epochs` nÃ­zko (1-3).

2. **ProblÃ©my s kvalitou dat** - Chyby v trÃ©novacÃ­ch datech = chyby v modelu. PeÄlivÄ› kontrolujte!

3. **Å patnÃ½ use case** - NepouÅ¾Ã­vejte fine-tuning pro znalosti. Na to je RAG.

4. **ZapomenutÃ­ System PromptÅ¯** - Fine-tuned modely stÃ¡le tÄ›Å¾Ã­ z jasnÃ½ch instrukcÃ­ pÅ™i inference.

</Callout>

---

## Holocron shrnutÃ­

<ConceptCard title="MistrovstvÃ­ Fine-Tuningu" icon="ğŸ’">

### ğŸ”‘ KlÃ­ÄovÃ© poznatky

* **Co to je:** Aktualizace vah modelu pro nauÄenÃ­ novÃ½ch dovednostÃ­, stylÅ¯ nebo formÃ¡tÅ¯
* **Kdy pouÅ¾Ã­t:** Pouze po selhÃ¡nÃ­ Prompt Engineeringu A RAG
* **ZÃ¡kladnÃ­ poÅ¾adavek:** KvalitnÃ­, ÄistÃ¡ data ve formÃ¡tu JSONL
* **Proces:** NahrÃ¡t soubor â†’ VytvoÅ™it Job â†’ Sledovat â†’ PouÅ¾Ã­t novÃ½ model
* **NÃ¡klady:** TrÃ©nink + prÃ©miovÃ© pouÅ¾itÃ­ = investice do specializace

### ğŸ§  RozhodovacÃ­ rÃ¡mec

```
PotÅ™ebuji novÃ© znalosti? â†’ RAG
PotÅ™ebuji konzistentnÃ­ styl? â†’ Fine-Tuning
PotÅ™ebuji rychlÃ½ experiment? â†’ Prompt Engineering
```

### ğŸ›¡ï¸ BezpeÄnostnÃ­ varovÃ¡nÃ­

* **Overfitting:** PouÅ¾ijte rozmanitÃ¡ data, mÃ¡lo epoch
* **Kvalita dat:** VÃ­ce lidskÃ½ch recenzentÅ¯
* **RÅ¯st nÃ¡kladÅ¯:** Sledujte pouÅ¾itÃ­, nastavte upozornÄ›nÃ­

</ConceptCard>

---

<Callout type="success">
**Gratulujeme!** DokonÄili jste lekci Fine-Tuningu. NynÃ­ rozumÃ­te kdy pouÅ¾Ã­t tuto mocnou techniku - a co je dÅ¯leÅ¾itÄ›jÅ¡Ã­, kdy NE. Pamatujte: nejlepÅ¡Ã­ fine-tuning job je Äasto ten, kterÃ½ nepotÅ™ebujete udÄ›lat.
</Callout>
